\setcounter{chapter}{4}

\chapter{Orthogonality}
\section{Orthogonality in \Rn}
\textit{Definition.} A set of vectors $\{$\vk$\}$ in $\mathbb{R}^n$ is an \textbf{orthogonal set} if all pairs of distinct vectors in the set are orthogonal, that is \begin{equation*}
	\textbf{v}_i \cdot \textbf{v}_j = 0 \mbox{ for } i \neq j, 1 \le i, j \le k
\end{equation*}

\textit{Definition.} An \textbf{orthogonal basis} for a subspace $W$ of \Rn is a basis of $W$ which is an orthogonal set. \\

\textit{Definition.} A set of vectors in $\mathbb{R}^n$ is an \textbf{orthonormal set} if it is an orthogonal set of unit vectors. Similarly, an \textbf{orthonormal basis} for a subspace $W$ of $\mathbb{R}^n$ is a basis of $W$ that is an orthonormal set. \\

\textit{Definition.} An \textbf{orthogonal matrix} is a square matrix whose columns form an \textit{orthonormal} set.

\begin{theorem}
	If $\{ \textbf{v}_1, \cdots, \textbf{v}_k \}$ is an orthogonal set of nonzero vectors in \Rn, then these vectors are linearly independent.
\end{theorem}
\begin{proof}
	Consider the equation $c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k = \textbf{0}$. For any $1 \le i \le k$, \begin{align*}
		(c_1\textbf{v}_1 + \cdots + c_k\textbf{v}_k) \cdot \textbf{v}_i &= c_1(\textbf{v}_1 \cdot \textbf{v}_i) + \cdots + c_k(\textbf{v}_k \cdot \textbf{v}_i) \\
		&= c_i(\textbf{v}_i \cdot \textbf{v}_i) = \textbf{0} \cdot \textbf{v} = 0
	\end{align*}
	Since $\textbf{v}_i$ is a nonzero vector, $c_i = 0$. Thus $c_1 = c_2 = \cdots = c_k = 0$. Therefore, \vkset is a linearly independent set of vectors.
\end{proof}

\begin{theorem}
	Let $\{ \textbf{v}_1, \cdots, \textbf{v}_k \}$ be an orthogonal basis for a subspace $ W $ of $\mathbb{R}^n$ and let $ \textbf{w} $ be any vector in $ W $. Then the unique scalars $ c_{1},  \cdots, c_{k} $ such that
	$$ \textbf{w} = c_{1}\textbf{v}_{1} + \cdots +  c_{k}\textbf{v}_{k} $$
	are given by
	\begin{center}
		$c_{i}=\dfrac{\textbf{w}\cdot\textbf{v}_{i}}{\textbf{v}_{i}\cdot\textbf{v}_{i}} $ for $ i = 1, \cdots ,k $
	\end{center}
\end{theorem}
\begin{proof}
	Since $\{ \textbf{v}_1, \cdots, \textbf{v}_k \}$ is a basis for $W$, the scalars $c_1, \cdots, c_k$ which satisfies $\textbf{w} = c_1\textbf{v}_1 + \cdots + c_k\textbf{v}_k$ are unique. (Remind Theorem 3.29.) For any $1 \le i \le k$, \begin{align*}
		\textbf{w} \cdot \textbf{v}_i &= (c_1\textbf{v}_1 + \cdots + c_k\textbf{v}_k) \cdot \textbf{v}_i \\
		&= c_1(\textbf{v}_1 \cdot \textbf{v}_i) + \cdots + c_k(\textbf{v}_k \cdot \textbf{v}_i) \\
		&= c_i(\textbf{v}_i \cdot \textbf{v}_i) \\
		&\therefore c_i = \frac{\textbf{w} \cdot \textbf{v}_i}{\textbf{v}_i \cdot \textbf{v}_i} \mbox{ for } i = 1, 2, \cdots, k
	\end{align*}
\end{proof}

\begin{theorem}
	Let $\{\textbf{q}_1, \textbf{q}_2, \cdots, \textbf{q}_k \}$ be an orthonormal basis for a subspace $W$ of \Rn, and let $\textbf{w}$ be any vector in $W$. Then $\textbf{w}$ can be represented as the linear combination of vectors in such basis as \begin{equation*}
		\textbf{w} = (\textbf{w} \cdot \textbf{q}_1)\textbf{q}_1 + (\textbf{w} \cdot \textbf{q}_2)\textbf{q}_2 + \cdots + (\textbf{w} \cdot\textbf{q}_k)\textbf{q}_k
	\end{equation*} and the representation is unique.
\end{theorem}
\begin{proof}
	By Theorem 5.2, the representation of $\textbf{w}$ as the linear combination of $\{\textbf{q}_1, \textbf{q}_2, \cdots, \textbf{q}_k \}$ is unique, and is expressed as \begin{equation*}
		\textbf{w} = \left ( \frac{\textbf{w}\cdot\textbf{q}_1}{\textbf{q}_1\cdot\textbf{q}_1} \right )\textbf{q}_1 + \left (\frac{\textbf{w}\cdot\textbf{q}_2}{\textbf{q}_2\cdot\textbf{q}_2} \right )\textbf{q}_2 + \cdots + \left (\frac{\textbf{w}\cdot\textbf{q}_k}{\textbf{q}_k\cdot\textbf{q}_k} \right )\textbf{q}_k
	\end{equation*} Since $\{\textbf{q}_1, \textbf{q}_2, \cdots, \textbf{q}_k \}$ is an orthonormal set, $\textbf{q}_i \cdot \textbf{q}_i = 1$ for all $1 \le i \le k$. Therefore, \begin{equation*}
		\textbf{w} = (\textbf{w}\cdot\textbf{q}_1)\textbf{q}_1 + \cdots + (\textbf{w}\cdot\textbf{q}_k)\textbf{q}_k
	\end{equation*}
\end{proof}

\begin{theorem}
	The columns of an $m \times n$ matrix $Q$ form an orthonormal set if and only if $Q^TQ = I_n$.
\end{theorem}
\begin{proof}
	Let $Q = \begin{bmatrix}
		\textbf{q}_1 & \textbf{q}_2 & \cdots & \textbf{q}_n
	\end{bmatrix}$, then \begin{equation*}
		(Q^TQ)_{ij} = \textbf{q}_i \cdot \textbf{q}_j
	\end{equation*}
	Therefore, $\{\textbf{q}_1, \textbf{q}_2, \cdots, \textbf{q}_k \}$ is an orthonormal set if and only if \begin{equation*}
		\textbf{q}_i \cdot \textbf{q}_j = (Q^TQ)_{ij} = \begin{cases}
			0 \mbox{ if } i \neq j \\
			1 \mbox{ if } i = j
		\end{cases}
	\end{equation*}
	which holds if and only if $Q^TQ = I_n$.
\end{proof}

\begin{theorem}
	A square matrix $Q$ is orthogonal if and only if $\inv{Q} = Q^T$.
\end{theorem}
\begin{proof}
	By Theorem 5.4, $Q$ is orthogonal if and only if $Q^TQ = I_n$, which holds if and only if $Q^T = \inv{Q}$.
\end{proof}

\begin{theorem}
	Let $Q$ be an \nbyn matrix. The following propositions are equivalent:
	\begin{enumerate}
		\item $Q$ is orthogonal.
		\item $\Vert Q\textbf{x} \Vert = \Vert \textbf{x} \Vert$
		\item $Q\textbf{x} \cdot Q\textbf{y} = \textbf{x} \cdot \textbf{y}$ for any $\textbf{x} \in$ $\mathbb{R}^n$ and $\textbf{y} \in$ \Rn.
	\end{enumerate}
\end{theorem}
\begin{proof}
	(a $\Rightarrow$ c) Suppose that $Q$ is an orthogonal matrix. By Theorem 5.4, $Q^TQ = I_n$. Therefore, for any $\textbf{x} \in$ \Rn and $\textbf{y} \in$ \Rn, \begin{equation*}
		Q\textbf{x} \cdot Q\textbf{y} = (Q\textbf{x})^TQ\textbf{y} = \textbf{x}^TQ^TQ\textbf{y} = \textbf{x}^T\textbf{y} = \textbf{x}\cdot\textbf{y}
	\end{equation*}
	
	(c $\Rightarrow$ a) Let $Q = \begin{bmatrix}
		\textbf{q}_1 & \textbf{q}_2 & \cdots & \textbf{q}_n
	\end{bmatrix}$, then $\textbf{q}_i = Q\textbf{e}_i$. Then, \begin{equation*}
		(Q^TQ)_{ij} = \textbf{q}_i \cdot \textbf{q}_j = (Q\textbf{e}_i) \cdot (Q\textbf{e}_j) = \textbf{e}_i \cdot \textbf{e}_j = \begin{cases}
			0 \mbox{ if } i \neq j \\
			1 \mbox{ if } i = j
		\end{cases}
	\end{equation*}
	which shows that $Q$ is an orthogonal matrix since $Q^TQ = I_n$. \\
	
	(b $\Rightarrow$ c) \begin{align*}
		\textbf{x} \cdot \textbf{y} &= \frac{1}{4}({\Vert \textbf{x} + \textbf{y} \Vert}^2 - {\Vert \textbf{x} - \textbf{y} \Vert}^2) \\
		&= \frac{1}{4}({\Vert Q(\textbf{x} + \textbf{y}) \Vert}^2 - {\Vert Q(\textbf{x} - \textbf{y}) \Vert}^2) \\
		&= \frac{1}{4}({\Vert Q\textbf{x} + Q\textbf{y} \Vert}^2 - {\Vert Q\textbf{x} - Q\textbf{y} \Vert}^2) \\
		&= (Q\textbf{x}) \cdot (Q\textbf{y})
	\end{align*}
	
	(c $\Rightarrow$ b) \begin{equation*}
		\Vert Q\textbf{x} \Vert = \sqrt{(Q\textbf{x})\cdot(Q\textbf{x})} = \sqrt{\textbf{x}\cdot\textbf{x}} = \Vert \textbf{x} \Vert
	\end{equation*}
\end{proof}

\begin{theorem}
	If $Q$ is an orthogonal matrix, then its rows form an orthonormal set.
\end{theorem}
\begin{proof}
	By Theorem 5.5, $Q^T = \inv{Q}$. Since $\inv{(Q^T)} = \inv{(\inv{Q})} = Q = (Q^T)^T$, $Q^T$ is also an orthogonal matrix by Theorem 5.4. Therefore, the columns of $Q^T$, which are rows of $Q$, form an orthonormal set.
\end{proof}

\begin{theorem}
	Let $Q$ be an orthogonal matrix.
	\begin{enumerate}
		\item $\inv{Q}$ is orthogonal.
		\item $\vert \det{Q} \vert = 1$
		\item If $\lambda$ is an eigenvalue of $Q$, then $\vert \lambda \vert = 1$.
		\item If $Q_1$ and $Q_2$ are orthogonal \nbyn matrices, then $Q_1Q_2$ is also an orthogonal matrix.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $Q$ be an orthogonal matrix.
	\begin{enumerate}
		\item \textbf{(Exercise 5.1 22)} Since $Q$ is an orthogonal matrix, $Q^T = \inv{Q}$. So \begin{equation*}
			\inv{(\inv{Q})} = \inv{(Q^T)} = {(\inv{Q})}^T
		\end{equation*}
		By Theorem 5.4, $\inv{Q}$ is also orthogonal.
		\item \textbf{(Exercise 5.1 23)} Since $Q^TQ = I_n$, \begin{equation*}
			\det{I_n} = \det{(Q^TQ)} = (\det{Q^T})(\det{Q}) = {(\det{Q})}^2 = 1
		\end{equation*}
		Therefore, $\vert \det{Q} \vert = 1$.
		\item Let $\lambda$ be a eigenvalue of $Q$ with corresponding eigenvector $\textbf{v}$. By Theorem 5.6(b), \begin{equation*}
			\Vert \textbf{v} \Vert = \Vert Q\textbf{v} \Vert = \Vert \lambda\textbf{v} \Vert = \vert \lambda \vert \Vert \textbf{v} \Vert
		\end{equation*}
		Therefore $\vert \lambda \vert = 1$.
		\item \textbf{(Exercise 5.1 24)} Let $Q_1$ and $Q_2$ be orthogonal matrices of the same size. Since $\inv{Q_1} = Q_1^T$ and $\inv{Q_2} = Q_2^T$. \begin{equation*}
			\inv{(Q_1Q_2)} = \inv{Q_2}\inv{Q_1} = Q_2^TQ_1^T = (Q_1Q_2)^T
		\end{equation*}
		Therefore $Q_1Q_2$ is also an orthogonal matrix.
	\end{enumerate}
\end{proof}

\section{Orthogonal Complements and Orthogonal Projections}

\textit{Definition.} Let $W$ be a subspace of \Rn. A vector \textbf{$\textbf{v}$ in $\mathbb{R}^n$ is orthogonal to $W$} if $\textbf{v}$ is orthogonal to every vector in $W$. The set of all vectors that are orthogonal to $W$ is the \textbf{orthogonal complement} of $W$, denoted by $W^\perp$.

\begin{theorem}
	Let $W$ be a subspace of \Rn. The following propositions hold:
	\begin{enumerate}
		\item $W^\perp$ is a subspace of \Rn.
		\item $(W^\perp)^\perp = W$
		\item $W \cap W^\perp = \{\textbf{0}\}$
		\item If $W = $ span($\textbf{w}_1, \cdots, \textbf{w}_k$), then $\textbf{v} \in W^\perp$ if and only if $\textbf{v}\cdot\textbf{w}_i = 0$ for all $i$ from 1 to $k$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $W$ be a subspace of \Rn.
	\begin{enumerate}
		\item (i) $\textbf{0} \cdot \textbf{w} = 0$ for every $\textbf{w} \in  W$, so $\textbf{0} \in W^\perp$. \\
		
		Let $\textbf{u} \in W$ and $\textbf{v} \in W$, and $c$ a scalar. Then for any $\textbf{w} \in W$, $\textbf{u} \cdot \textbf{w} = \textbf{v} \cdot \textbf{w} = 0$. \\
		(ii) $(\textbf{u} + \textbf{v}) \cdot \textbf{w} = \textbf{u} \cdot \textbf{w} + \textbf{v} \cdot \textbf{w} = 0$ \\
		
		(iii) $(c\textbf{u})\cdot\textbf{w} = c(\textbf{u}\cdot\textbf{w}) = 0$
		\item (i) Let $\textbf{w}$ be a vector in $W$, then for every vector $\textbf{w}^\perp$ in $W^\perp$, $\textbf{w} \cdot \textbf{w}^\perp = 0$. This implies that $\textbf{w} \in (W^\perp)^\perp$, so $W \subseteq (W^\perp)^\perp$. \\
		
		(ii) Now let $\textbf{v}$ be a vector in $(W^\perp)^\perp$. By Orthogonal Decomposition Theorem (Theorem 5.11), there are unique vectors $\textbf{w} \in W$ and $\textbf{w}^\perp \in W^\perp$ such that $\textbf{v} = \textbf{w} + \textbf{w}^\perp$. Since $\textbf{v} \in (W^\perp)^\perp$. $\textbf{v}$ and $\textbf{w}^\perp$ are orthogonal, so \begin{equation*}
			\textbf{v} \cdot \textbf{w}^\perp = (\textbf{w} + \textbf{w}^\perp) \cdot \textbf{w}^\perp = \textbf{w} \cdot \textbf{w}^\perp + \textbf{w}^\perp \cdot \textbf{w}^\perp = \textbf{w}^\perp \cdot \textbf{w}^\perp = 0
		\end{equation*}
		Therefore $\textbf{w}^\perp = \textbf{0}$, which indicates $\textbf{v} = \textbf{w} \in W$. Thus, $(W^\perp)^\perp \subseteq W$. \\
		
		By (i) and (ii), $W = (W^\perp)^\perp$.
		\item \textbf{(Exercise 5.2 23)} Suppose that $\textbf{v} \in W \cap W^\perp$. Then $\textbf{v} \cdot \textbf{v} = 0$, which is true if and only if $\textbf{v} = \textbf{0}$. Therefore, $W \cap W^\perp = \{\textbf{0}\}$.
		\item \textbf{(Exercise 5.2 24)} ($\Rightarrow$) Since $\textbf{v} \in W^\perp$, $\textbf{v}$ is orthogonal with every vector in $W$, including the vectors $\textbf{w}_1, \cdots, \textbf{w}_k$. \\
		
		($\Leftarrow$) For any vector $\textbf{w} \in W$, there exist scalars $c_1, \cdots, c_k$ such that \begin{equation*}
			\textbf{w} = c_1\textbf{w}_1 + \cdots + c_k\textbf{w}_k
		\end{equation*} Since $\textbf{v}$ is orthogonal to $\textbf{w}_1, \cdots, \textbf{w}_k$, \begin{equation*}
			\textbf{w} \cdot \textbf{v} = (c_1\textbf{w}_1 + \cdots + c_k\textbf{w}_k) \cdot \textbf{v} = c_1(\textbf{w}_1 \cdot \textbf{v}) + \cdots + c_k(\textbf{w}_k \cdot \textbf{v}) = 0
		\end{equation*} so $\textbf{w}$ is orthogonal to $\textbf{v}$. Therefore, $\textbf{v} \in W^\perp$.
	\end{enumerate}
\end{proof}

\begin{theorem}
	Let $A$ be an $m \times n$ matrix. Then the orthogonal complement of the row space of $A$ is the null space of $A$, and the orthogonal complement of the column space of $A$ is the null space of $A^T$. That is, \begin{equation*}
		(\textnormal{row}(A))^\perp = \textnormal{null}(A) \mbox{ and } (\textnormal{col}(A))^\perp = \textnormal{null}(A^T)
	\end{equation*}
\end{theorem}
\begin{proof}
	Let $\textbf{v}$ be a vector in \Rn. Then $\textbf{v}$ satisfies the equation $A\textbf{x} = \textbf{0}$ (which means $\textbf{v} \in $ null($A$)) if and only if $\textbf{v}$ is orthogonal to every rows of $A$. Since the rows of $A$ span the row space of $A$, by Theorem 5.9(d), this is true if and only if $\textbf{v} \in (\textnormal{row}(A))^\perp$. \\
	
	Let $\textbf{v}$ be a vector in \Rm. Then $\textbf{v}$ satisfies the equation $A^T\textbf{x} = \textbf{0}$ (which means $\textbf{v} \in $ null($A^T$)) if and only if $\textbf{v}$ is orthogonal to every rows of $A^T$, which are columns of $A$. Since the columns of $A$ span the column space of $A$, by Theorem 5.9(d), this is true if and only if $\textbf{v} \in (\textnormal{col}(A))^\perp$. \\
	
	Also note that \begin{equation*}
		(\textnormal{col}(A))^\perp = (\textnormal{row}(A^T))^\perp = \textnormal{null}(A^T)
	\end{equation*}
\end{proof}

The subspaces row($A$), col($A$), null($A$), null($A^T$) are \textbf{fundamental subspaces} of the matrix $A$. \\

\textit{Definition.} Let $W$ be a subspace of \Rn and let $\{\textbf{u}_1, \cdots, \textbf{u}_k\}$ be an orthogonal basis for $W$. For any vector $\textbf{v} \in$ \Rn, the orthogonal projection of $\textbf{v}$ onto $W$ is defined as \begin{equation*}
	\textnormal{proj}_W(\textbf{v}) = \left (\frac{\textbf{u}_1 \cdot \textbf{v}}{\textbf{u}_1 \cdot \textbf{u}_1} \right )\textbf{u}_1 + \cdots + \left (\frac{\textbf{u}_k \cdot \textbf{v}}{\textbf{u}_k \cdot \textbf{u}_k} \right )\textbf{u}_k
\end{equation*}
and the component of $\textbf{v}$ orthogonal to $W$ is defined as \begin{equation*}
	\textnormal{perp}_{W}{\textbf{v}} = \textbf{v} - \textnormal{proj}_W(\textbf{v})
\end{equation*}

Note that proj$_W(\textbf{v})$ = proj$_{\textbf{u}_1}(\textbf{v}) + \cdots + $ proj$_{\textbf{u}_k}(\textbf{v})$.

\begin{theorem}[The Orthogonal Decomposition Theorem]
	Let $W$ be a subspace of $\mathbb{R}^n$ and let $\textbf{v}$ be a vector in \Rn. Then there are unique vectors $\textbf{w} \in W$ and $\textbf{w}^\perp \in W^\perp$ such that \begin{equation*}
		\textbf{v} = \textbf{w} + \textbf{w}^\perp
	\end{equation*}
	which is an orthogonal decomposition of $\textbf{v}$ with respect to $W$.
\end{theorem}
\begin{proof}
	\textit{Existence.} Choose an orthogonal basis $\{\textbf{u}_1, \cdots, \textbf{u}_k \}$ for $W$, and let $\textbf{w} = $ proj$_W(\textbf{v})$ and $\textbf{w}^\perp = $ perp$_W(\textbf{v})$. Then \begin{equation*}
		\textbf{w} + \textbf{w}^\perp = \textnormal{proj}_W(\textbf{v}) + (\textbf{v} - \textnormal{proj}_W(\textbf{v})) = \textbf{v}
	\end{equation*} Since $\textbf{w}$ is a linear combination of $\textbf{u}_1, \cdots, \textbf{u}_k$, $\textbf{w} \in W$. Also, \begin{align*}
		\textbf{u}_i \cdot \textbf{w}^\perp &= \textbf{u}_i \cdot \textnormal{perp}_W(\textbf{v}) \\
		&= \textbf{u}_i \cdot \left (\textbf{v} - \left (\frac{\textbf{u}_1 \cdot \textbf{v}}{\textbf{u}_1 \cdot \textbf{u}_1} \right )\textbf{u}_1 - \cdots - \left (\frac{\textbf{u}_k \cdot \textbf{v}}{\textbf{u}_k \cdot \textbf{u}_k} \right )\textbf{u}_k \right ) \\
		&= \textbf{u}_i \cdot \textbf{v} - \left (\frac{\textbf{u}_1 \cdot \textbf{v}}{\textbf{u}_1 \cdot \textbf{u}_1} \right )(\textbf{u}_1 \cdot \textbf{u}_i) - \cdots - \left (\frac{\textbf{u}_k \cdot \textbf{v}}{\textbf{u}_k \cdot \textbf{u}_k} \right )(\textbf{u}_k \cdot \textbf{u}_i) \\
		&= \textbf{u}_i \cdot \textbf{v} - \left (\frac{\textbf{u}_i \cdot \textbf{v}}{\textbf{u}_i \cdot \textbf{u}_i} \right )(\textbf{u}_i \cdot \textbf{u}_i) \\
		&= \textbf{u}_i \cdot \textbf{v} - \textbf{u}_i \cdot \textbf{v} = 0
	\end{align*} By Theorem 5.9(d), $\textbf{w}^\perp \in W^\perp$. Therefore, $\textbf{w}$ and $\textbf{w}^\perp$ satisfies the condition.\\
	
	\textit{Uniqueness.} Suppose that there are another vectors $\textbf{w}_1$ and $\textbf{w}^\perp_1$ which satisfy the condition: so $\textbf{v} = \textbf{w}_1 + \textbf{w}^\perp_1$, $\textbf{w}_1 \in W$ and $\textbf{w}^\perp_1 \in W^\perp$. Then $\textbf{w} + \textbf{w}^\perp = \textbf{w}_1 + \textbf{w}^\perp_1 = \textbf{v}$, so \begin{equation*}
		\textbf{w} - \textbf{w}_1 = \textbf{w}_1^\perp - \textbf{w}^\perp
	\end{equation*}
	Since $\textbf{w} - \textbf{w}_1 \in W$ and $\textbf{w}_1^\perp - \textbf{w}^\perp \in W^\perp$, it should be $\textbf{0}$ since the only common vector of $W$ and $W^\perp$ is the zero vector. Therefore, $\textbf{w} = \textbf{w}_1$ and $\textbf{w}^\perp = \textbf{w}^\perp_1$.
\end{proof}

\textit{Note.} Orthogonal Decomposition Theorem은 직교성분분해가 존재하고 유일하다는 정리이지만, 그 직교성분분해가 $\textbf{v} = \textnormal{proj}_W(\textbf{v}) + \textnormal{perp}_W(\textbf{v})$ 인 것으로 확장시켜도 문제 없다. 또한, orthogonal basis $\{ \textbf{u}_1, \cdots, \textbf{u}_k \}$ 가 변해도 orthogonal decomposition은 변화 없다.

\begin{corollary}
	If $W$ is a subspace of \Rn, then \begin{equation*}
		(W^\perp)^\perp = W
	\end{equation*}
\end{corollary}
\begin{proof}
	Corollary 5.12 is equal to Theorem 5.9(b).
\end{proof}

\begin{theorem}
	If $W$ is a subspace of \Rn, then \begin{equation*}
		\dim{W} + \dim{W^\perp} = n
	\end{equation*}
\end{theorem}
\begin{proof}
	Let $\mathcal{B}_W = \{\textbf{u}_1, \cdots, \textbf{u}_k \}$ be an orthogonal basis for $W$ and let $\mathcal{B}_{W^\perp} = \{\textbf{v}_1, \cdots, \textbf{v}_l \}$ be an orthogonal basis for $W^\perp$. Let $\mathcal{B} = \mathcal{B}_W \cup \mathcal{B}_{W^\perp}$. Since $\textbf{u}_i \cdot \textbf{v}_j = 0$ for every $i$ and $j$, $\mathcal{B}$ is an orthogonal set of nonzero vectors, which is also a linearly independent set. (Theorem 5.1) By Theorem 5.11, for any $\textbf{v} \in \Rnn$, there exist $\textbf{w} \in W$ and $\textbf{w}^\perp \in W^\perp$ such that $\textbf{v} = \textbf{w} + \textbf{w}^\perp$, which indicates that $\textbf{v} \in$ span($\mathcal{B}$). Thus, span($\mathcal{B}$) = \Rn, so $\mathcal{B}$ is an orthogonal basis for \Rn. Therefore, \begin{equation*}
		\dim{\Rnn} = n = k + l = \dim{W} + \dim{W^\perp}
	\end{equation*}
\end{proof}

\begin{corollary}
	If $A$ is an $m \times n$ matrix, \begin{equation*}
		\textnormal{rank}(A) + \textnormal{nullity}(A) = n
	\end{equation*}
\end{corollary}
\begin{proof}
	By Theorem 5.13, \begin{equation*}
		\dim{(\textnormal{row}(A))} + \dim{(\textnormal{row}(A))^\perp} = \dim{(\textnormal{row}(A))} + \dim{(\textnormal{null}(A))} = \textnormal{rank}(A) + \textnormal{nullity}(A)
	\end{equation*}
\end{proof}

\newpage
\section{The $ QR $ Factorization}

\begin{theorem}[The Gram-Schmidt Process]
	Let $\{\textbf{x}_1, \cdots, \textbf{x}_k \}$ be a basis for a subspace $W$ of \Rn. Define $\textbf{v}_1, \cdots, \textbf{v}_k$ and $W_1, \cdots, W_k$ as following:
	\begin{table}[H]
		\begin{center}
			\begin{tabular}{cc}
				$\textbf{v}_1 = \textbf{x}_1$ & $W_1$ = span($\textbf{x}_1$) \\
				& \\
				$\textbf{v}_2 = \textbf{x}_2 - \left (\frac{\textbf{x}_2 \cdot \textbf{v}_1}{\textbf{v}_1 \cdot \textbf{v}_1} \right )\textbf{v}_1 = \textbf{x}_2 - \textnormal{proj}_{W_1}(\textbf{x}_2)$ & $W_2$ = span($\textbf{x}_1, \textbf{x}_2$) \\
				& \\
				$\vdots$ & $\vdots$ \\
				& \\
				$\textbf{v}_k = \textbf{x}_k - \left (\frac{\textbf{x}_k \cdot \textbf{v}_1}{\textbf{v}_1 \cdot \textbf{v}_1}\right )\textbf{v}_1 - \cdots - \left (\frac{\textbf{x}_k \cdot \textbf{v}_{k-1}}{\textbf{v}_{k-1} \cdot \textbf{v}_{k-1}} \right )\textbf{v}_{k-1} = \textbf{x}_k - \textnormal{proj}_{W_{k-1}}(\textbf{x}_k)$ & $W_k$ = span($\textbf{x}_1, \cdots, \textbf{x}_k$)
			\end{tabular}
		\end{center}
	\end{table}
	Then $\{\textbf{v}_1, \cdots, \textbf{v}_i \}$ is an orthogonal basis for $W_i$.
\end{theorem}
\begin{proof}
	We prove the theorem using the mathematical induction. \\
	
	(i) Since $\textbf{v}_1 = \textbf{x}_1$, $\{\textbf{v}_1 \}$ is an orthogonal basis for $W_1$. \\
	
	(ii) Suppose that $\{\textbf{v}_1, \cdots, \textbf{v}_i \}$ is an orthogonal basis for $W_i = \textnormal{span}(\textbf{x}_1, \cdots, \textbf{x}_i)$.  Let \begin{equation*}
	\textbf{v}_{i+1} = \textbf{x}_{i+1} - \left (\frac{\textbf{x}_{i+1} \cdot \textbf{v}_1}{\textbf{v}_1 \cdot \textbf{v}_1} \right )\textbf{v}_1 - \cdots - \left (\frac{\textbf{x}_{i+1} \cdot \textbf{v}_i}{\textbf{v}_i \cdot \textbf{v}_i} \right)\textbf{v}_i = \textbf{x}_{i+1} - \textnormal{proj}_{W_i}(\textbf{x}_{i+1}) = \textnormal{perp}_{W_i}(\textbf{x}_{i+1})
	\end{equation*}
	which is orthogonal to $W_i$. {\color{blue}(Orthogonal Decomposition Theorem 증명 과정에서 보였다.)} \\
	Also, $\textbf{v}_{i+1} \neq \textbf{0}$, because if $\textbf{v}_{i+1} = \textbf{0}$, $\textbf{x}_{i+1} = \textnormal{proj}_{W_i}(\textbf{x}_{i+1}) \in W_i = \textnormal{span}(\textbf{x}_1, \cdots, \textbf{x}_i)$, which is impossible since $\{\textbf{x}_1, \cdots, \textbf{x}_{i+1} \}$ is a linearly independent set. {\color{blue}($\{\textbf{x}_1, \cdots, \textbf{x}_k \}$ 가 $W$의 기저이므로 선형독립이다.)} \\
	
	Since $\{\textbf{v}_1, \cdots, \textbf{v}_{i+1} \}$ is an orthogonal set of nonzero vectors, by Theorem 5.1, $\{\textbf{v}_1, \cdots, \textbf{v}_{i+1} \}$ is linearly independent set. Note that $\textbf{v}_1, \cdots, \textbf{v}_i \in W_i \subset W_{i+1}$, and $\textbf{v}_{i+1} \in W_{i+1}$. Since $\dim{W_{i+1}} = i+1$, $\{\textbf{v}_1, \cdots, \textbf{v}_{i+1} \}$ is an orthogonal, linearly independent set of $i+1$ vectors, so it forms an orthogonal basis for $W_{i+1}$. \\
	
	(i) and (ii) gives the proof for the Gram-Schmidt Process. \\
	
	{\color{blue}Orthogonal projection은 orthogonal basis가 존재할 때 정의할 수 있다는 점에서 (ii)의 귀납 가정이 필요하다는 사실을 숙지하자.}
\end{proof}

\begin{theorem}[The $QR$ Factorization]
	Let $A$ be an $m \times n$ matrix with linearly independent columns. Then there exists an $m \times n$ matrix $Q$ with orthogonal columns, and an invertible triangular matrix $R$.
\end{theorem}
\begin{proof}
	Let $A = \begin{bmatrix}
		\textbf{a}_1 & \cdots & \textbf{a}_n
	\end{bmatrix}$, and let $\{\textbf{q}_1, \cdots, \textbf{q}_n \}$ be the orthonormal basis of col($A$) obtained by Gram-Schmidt Process and normalization. By Theorem 5.15, $\{\textbf{q}_1, \cdots, \textbf{q}_i \}$ forms an orthonormal basis for span($\textbf{a}_1, \cdots, \textbf{a}_i$) for $i = 1, \cdots, n$. Thus, there exist scalars $r_{1i}, r_{2i}, \cdots, r_{ii}$ such that \begin{equation*}
		\textbf{a}_i = r_{1i}\textbf{q}_1 + r_{2i}\textbf{q}_2 + \cdots + r_{ii}\textbf{q}_i
	\end{equation*} where $r_{ii} \neq 0$. (If $r_{ii} = 0$, $\textbf{a}_i \in \textnormal{span}(\textbf{a}_1, \cdots, \textbf{a}_{i-1})$ so it is a contradiction.) Then \begin{align*}
		A &= \begin{bmatrix}
			\textbf{a}_1 & \textbf{a}_2 & \cdots & \textbf{a}_n
		\end{bmatrix} \\
		&= \begin{bmatrix}
			\textbf{q}_1 & \textbf{q}_2 & \cdots & \textbf{q}_n
		\end{bmatrix} \begin{bmatrix}
			r_{11} & r_{12} & \cdots & r_{1n} \\
			0      & r_{22} & \cdots & r_{2n} \\
			\vdots & \vdots &        & \vdots \\
			0      & 0      & \cdots & r_{nn}
		\end{bmatrix}
	\end{align*}
	Therefore $Q = \begin{bmatrix}
		\textbf{q}_1 & \textbf{q}_2 & \cdots & \textbf{q}_n
	\end{bmatrix}$ and $R = \begin{bmatrix}
	r_{11} & r_{12} & \cdots & r_{1n} \\
	0      & r_{22} & \cdots & r_{2n} \\
	\vdots & \vdots &        & \vdots \\
	0      & 0      & \cdots & r_{nn}
	\end{bmatrix}$ satisfies the condition.
\end{proof}

\section{Orthogonal Diagonalization of Symmetric Matrices}

\textit{Definition.} A square matrix $A$ is \textbf{orthogonally diagonalizable} if there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that $Q^TAQ = D$.

\begin{theorem}
	If $A$ is orthogonally diagonalizable, then $A$ is symmetric.
\end{theorem}
\begin{proof}
	If $A$ is orthogonally diagonalizable, then there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that $Q^TAQ = D$. Since $\inv{Q} = Q^T$, \begin{equation*}
		A = \inv{(Q^T)}D\inv{Q} = QDQ^T
	\end{equation*} thus \begin{equation*}
		A^T = (QDQ^T)^T = (Q^T)^TD^TQ^T = QDQ^T = A
	\end{equation*}
	Therefore $A$ is symmetric.
\end{proof}

\begin{theorem}
	If $A$ is a real symmetric matrix, then the eigenvalues of $A$ are real.
\end{theorem}
\begin{proof}
	Suppose that $\lambda$ is an eigenvalue of $A$ with corresponding eigenvector $\textbf{v}$. Since $A$ is a real matrix, $A = \overline{A}$, so \begin{equation*}
		A\overline{\textbf{v}} = \overline{A}\overline{\textbf{v}} = \overline{A\textbf{v}} = \overline{\lambda\textbf{v}} = \overline{\lambda}\overline{\textbf{v}}
	\end{equation*}
	Thus, \begin{align*}
		\lambda(\overline{\textbf{v}}^T\textbf{v}) &= \overline{\textbf{v}}^T(\lambda\textbf{v}) = \overline{\textbf{v}}^T(A\textbf{v}) \\
		&= (\overline{\textbf{v}}^TA)\textbf{v} = (\overline{\textbf{v}}^TA^T)\textbf{v} = (A\overline{\textbf{v}})^T\textbf{v} = (\overline{\lambda}\overline{\textbf{v}})^T\textbf{v} = \overline{\lambda}(\overline{\textbf{v}}^T\textbf{v})
	\end{align*}
	Since $\textbf{v} \neq \textbf{0}$, $\overline{\textbf{v}}^T\textbf{v} = \textbf{v} \cdot \textbf{v} \neq 0$. ({\color{blue} 5.4 끝의 Remarks 참조}) Therefore, $\lambda = \overline{\lambda}$, which indicates that $\lambda$ is real.
\end{proof}

\begin{theorem}
	If $A$ is a symmetric `real' matrix, then any two eigenvectors corresponding to distinct eigenvalues of $A$ are orthogonal.
\end{theorem}
\begin{proof}
	Let $\textbf{v}_1$ and $\textbf{v}_2$ be eigenvectors corresponding to the distinct eigenvalues $\lambda_1, \lambda_2$ of $A$. Then, \begin{align*}
		\lambda_1(\textbf{v}_1 \cdot \textbf{v}_2) &= (\lambda_1\textbf{v}_1) \cdot \textbf{v}_2 = (A\textbf{v}_1) \cdot \textbf{v}_2 = (A\textbf{v}_1)^T\textbf{v}_2 \\
		&= \textbf{v}_1^TA^T\textbf{v}_2 = \textbf{v}_1^T(A\textbf{v}_2) = \textbf{v}_1^T(\lambda_2\textbf{v}_2) = \lambda_2(\textbf{v}_1 \cdot \textbf{v}_2)
	\end{align*}
	Thus $(\lambda_1 - \lambda_2)(\textbf{v}_1 \cdot \textbf{v}_2) = 0$. Since $\lambda_1 - \lambda_2 \neq 0$, $\textbf{v}_1 \cdot \textbf{v}_2 = 0$. Therefore, $\textbf{v}_1$ and $\textbf{v}_2$ are orthogonal.
\end{proof}

\begin{theorem}[The Spectral Theorem]
	Let $A$ be an $n \times n$ real matrix. Then $A$ is symmetric if and only if it is orthogonally diagonalizable.
\end{theorem}
\begin{proof}
	($\Leftarrow$) Theorem 5.17. \\
	
	($\Rightarrow$) We prove the proposition through inductive process. \\
	(i) If $A$ is a $1 \times 1$ real matrix, then $A$ is diagonal form, so $A$ is diagonalizable since $A = IAI$. \\
	
	(ii) Suppose that every $n \times n$ matrices are orthogonally diagonalizable. {\color{blue}(귀납 가정)} \\
	
	Let $A$ be an $(n+1) \times (n+1)$ real matrix, and let $\lambda_1$ be an eigenvalue of $A$. By Theorem 5.18, $\lambda_1$ is real, so there exists an real eigenvalue $\textbf{v}_1$ corresponding to $\lambda_1$. {\color{blue}($\textbf{v}_1$이 항상 real vector가 아니라, $\lambda_1$에 대응되는 eigenvector들 중 real vector가 존재한다는 서술이 정확하다. 예를 들어, $I_2$의 eigenvalue는 $\lambda = 1$이지만 eigenvector는 $\textbf{v} = \begin{bmatrix} i \\ i \end{bmatrix}$처럼 real vector가 아닐 수도 있기 때문. real eigenvector가 존재한다는 것에 대한 증명은: ``$\textbf{v}_1$은 $(A-\lambda_1I)\textbf{x}=\textbf{0}$의 해이고, $A-\lambda_1I$가 real matrix이므로 실수해가 존재한다.'')} \\
	
	We may assume that $\textbf{v}$ is a unit vector, as if not, we can normalize it. Using the Gram-Schmidt Process, we can construct an orthonormal basis $\{\textbf{v}_1, \cdots, \textbf{v}_n \}$ of \Rn. Let $Q_1 = \begin{bmatrix}
		\textbf{v}_1 & \cdots & \textbf{v}_n
	\end{bmatrix}$ then $Q_1$ is an orthogonal matrix, and let \begin{align*}
		B = Q_1^TAQ_1 &= \begin{bmatrix}
			\textbf{v}_1^T \\ \textbf{v}_2^T \\ \vdots \\ \textbf{v}_n^T
		\end{bmatrix}A\begin{bmatrix}
			\textbf{v}_1 & \textbf{v}_2 & \cdots & \textbf{v}_n
		\end{bmatrix} = \begin{bmatrix}
			\textbf{v}_1^T \\ \textbf{v}_2^T \\ \vdots \\ \textbf{v}_n^T
		\end{bmatrix} \begin{bmatrix}
			A\textbf{v}_1 & A\textbf{v}_2 & \cdots & A\textbf{v}_n
		\end{bmatrix} = \\
		&= \begin{bmatrix}
			\textbf{v}_1^T \\ \textbf{v}_2^T \\ \vdots \\ \textbf{v}_n^T
		\end{bmatrix}\begin{bmatrix}
			\lambda_1\textbf{v}_1 & A\textbf{v}_2 & \cdots & A\textbf{v}_n
		\end{bmatrix} = \begin{bmatrix}
			\lambda_1 & ? \\ \textbf{0} & A_1
		\end{bmatrix}
	\end{align*} since $\textbf{v}_1^T(\lambda_1\textbf{v}_1) = \lambda_1(\textbf{v}_1 \cdot \textbf{v}_1) = \lambda_1$ and $\textbf{v}_i^T(\lambda_1\textbf{v}_1) = \lambda_1(\textbf{v}_i \cdot \textbf{v}_1) = 0$ if $i \neq 1$. Since \begin{equation*}
		B^T = (Q_1^TAQ_1)^T = Q_1^TA^T(Q_1^T)^T = Q_1^TAQ_1 = B
	\end{equation*} $B$ is symmetric real matrix as it is a product of real matrices, so $B = \begin{bmatrix}
		\lambda_1 & \textbf{0} \\ \textbf{0} & A_1
	\end{bmatrix}$ and $A_1$ is also a $n \times n$ symmetric  real matrix. By inductive hypothesis, $A_1$ is orthogonally diagonalizable, so there exists an $n \times n$ orthogonal matrix $P$ and $n \times n$ diagonal matrix $D$ such that $P^TA_1P = D$. \\
	
	Let $Q_2 = \begin{bmatrix}
		1 & \textbf{0} \\ \textbf{0} & P
	\end{bmatrix}$, then $Q_2$ is orthogonal since columns of $P$ are orthonormal, and also the first column is orthogonal to every other columns. Then, \begin{align*}
		(Q_1Q_2)^TA(Q_1Q_2) &= Q_2^TQ_1^TAQ_1Q_2 = Q_2^TBQ_2 \\
		&= \begin{bmatrix}
			1 & \textbf{0} \\ \textbf{0} & P^T
		\end{bmatrix}\begin{bmatrix}
			\lambda_1 & \textbf{0} \\ \textbf{0} & A_1
		\end{bmatrix}\begin{bmatrix}
			1 & \textbf{0} \\ \textbf{0} & P
		\end{bmatrix} = \begin{bmatrix}
			\lambda_1 & \textbf{0} \\ \textbf{0} & P^TA_1P
		\end{bmatrix} = \begin{bmatrix}
			\lambda_1 & \textbf{0} \\ \textbf{0} & D
		\end{bmatrix}
	\end{align*} where $Q_1Q_2$ is an orthogonal matrix. Therefore, $A$ is diagonalizable. \\
	
	(i) and (ii) gives the inductive proof that every symmetric real matrices are orthogonally diagonalizable.
\end{proof}

\textit{Remarks.} 아까 Theorem 5.18부터 갑자기 왜 real matrix인지, real eigenvalue인지 따지는 것에 대해 궁금했을 것 같다. 우리가 orthogonality, orthonormality 등등을 모두 \Rn에서 정의했기 때문에 생기는 일이다. \\
사실 두 vector $\textbf{u}, \textbf{v}$가 \Rn의 vector가 아니라 $\mathbb{C}^n$의 vector들이라면 dot product의 정의부터 다르다. $\mathbb{C}^n$에서 두 vector $\textbf{u}, \textbf{v}$의 dot product는 다음과 같이 정의된다.
\begin{align*}
	\textbf{u} = \begin{bmatrix}
		u_1 \\ u_2 \\ \cdots \\ u_n
	\end{bmatrix}, \textbf{v} = \begin{bmatrix}
		v_1 \\ v_2 \\ \cdots \\ v_n
	\end{bmatrix} \\
	\textbf{u} \cdot \textbf{v} = \sum_{i=1}^{n}u_i\overline{v_i} = {\overline{\textbf{v}}}^T\textbf{u}
\end{align*}
여기에서는 dot product의 `교환법칙'조차 성립하지 않는다. $\textbf{u} \cdot \textbf{v} = \overline{\textbf{v} \cdot \textbf{u}}$ 이다.
그래서 $\textbf{u}\cdot\textbf{v}=0$으로 정의되는 orthogonality의 성질도 달라지니까, 결론적으로 처음부터 다시 해야 한다는 말이다. 원한다면 해보고 Spectral Theorem을 복소수 범위까지 확장시켜 보자. \\
확장시켜 보면, $A = {\overline{A}}^T$일때 Spectral Theorem이 성립한다 카더라... \\

참고 문헌 : en.wikipedia.org/wiki/Spectral\_theorem
\iffalse
\section{Solution for Exercises from Chapter 5}
\textbf{(5.1 25)} \\
참고) Theorem 3.17이 $\inv{P} = P^T$라는 명제이고, 이 풀이의 일부는 이 정리의 증명에 해당한다. \\

If $P$ is a permutation matrix, then $P$ is a product of elementary matrices which are obtained by interchanging two rows of $I$. So \begin{equation*}
	P = E_1\cdots E_k
\end{equation*} where $E_1, \cdots, E_k$ are such elementary matrices. Note that for each $E_i$, $\inv{E_i} = E_i^T$. (Remind Theorem 3.11) Thus, \begin{equation*}
	\inv{P} = \inv{(E_1\cdots E_k)} = \inv{E_k}\cdots\inv{E_1} = E_k^T\cdots E_1^T = (E_1\cdots E_k)^T = P^T
\end{equation*}
Therefore, $P$ is orthogonal by Theorem 5.5. \\

\textit{Note.} $P$가 $n \times n$ 행렬일 때 $P$의 열들이 $\textbf{e}
_1, \cdots, \textbf{e}_n$의 재배열임을 이용하여 증명하는 방법도 있다. \\

\textbf{(5.1 26)} \\
Let $Q$ be an orthogonal matrix and let $Q'$ be the matrix obtained by rearranging the rows of $Q$. Then there exists a permutation matrix $P$ such that $Q' = PQ$. (Remind Theorem 3.10) Since $P$ is orthogonal by Exercise 5.1 25, $Q'$ is also orthogonal by Theorem 5.8(d). \\

\textbf{(5.1 27)} \\
Let $\textbf{x}$ and $\textbf{y}$ be the vectors in $\mathbb{R}^2$. If $\theta$ is the angle between $\textbf{x}$ and $\textbf{y}$, \begin{equation*}
	\cos \theta = \frac{\textbf{x} \cdot \textbf{y}}{\Vert \textbf{x} \Vert \Vert \textbf{y} \Vert}
\end{equation*}
Let $Q$ be an orthogonal $2 \times 2$ matrix. Similarly, if $\theta'$ is the angle between $Q\textbf{x}$ and $Q\textbf{y}$, \begin{equation*}
	\cos \theta ' = \frac{(Q\textbf{x}) \cdot (Q\textbf{y})}{\Vert Q\textbf{x} \Vert \Vert Q\textbf{y} \Vert} = \frac{\textbf{x} \cdot \textbf{y}}{\Vert \textbf{x} \Vert \Vert \textbf{y} \Vert} = \cos \theta
\end{equation*} by Theorem 5.6(b) and Theorem 5.6(c). Therefore, $\theta = \theta '$. \\

\textbf{(5.1 28)} \\
(a) Suppose that $Q = \begin{bmatrix}
	a & c \\ b & d
\end{bmatrix}$ is an orthogonal matrix. Since the columns of $Q$ are orthonormal set of vectors, \begin{equation*}
	\left \Vert \begin{bmatrix}
		a \\ b
	\end{bmatrix} \right \Vert = \sqrt{a^2 + b^2} = 1, \left \Vert \begin{bmatrix}
		c \\ d
	\end{bmatrix} \right \Vert = \sqrt{c^2 + d^2} = 1, \begin{bmatrix}
		a \\ b
	\end{bmatrix} \cdot \begin{bmatrix}
		c \\ d
	\end{bmatrix} = ac + bd = 0
\end{equation*}
From $ac + bd = 0$,
\begin{align*}
	a^2c^2 = b^2d^2 = (1 - &a^2)(1 - c^2) = a^2c^2 - a^2 - c^2 + 1 \\
	&\therefore a^2 + c^2 = 1
\end{align*}
Since $a^2 + b^2 = 1$ and $c^2 + d^2 = 1$, $b^2 = c^2$ and $a^2 = d^2$. From $ab + cd = 0$, if $c = b$, then $d = -a$ and if $c = -b$, then $d = a$. Therefore, \begin{equation*}
	\begin{bmatrix}
		a & c \\ b & d
	\end{bmatrix} = \begin{bmatrix}
		a & b \\ b & -a
	\end{bmatrix} \mbox{ or } \begin{bmatrix}
		a & b \\ c & d
	\end{bmatrix} = \begin{bmatrix}
		a & -b \\ b & a
	\end{bmatrix}
\end{equation*}

(b) By (a), every orthogonal $2 \times 2$ matrix has the form \begin{equation*}
	\begin{bmatrix}
		a & b \\ b & -a
	\end{bmatrix} \mbox{ or } \begin{bmatrix}
		a & -b \\ b & a
	\end{bmatrix}
\end{equation*} where $a^2 + b^2 = 1$. So there exists $\theta \in [0, 2\pi)$ such that $a = \cos \theta$ and $b = \sin \theta$, then \begin{equation*}
	\begin{bmatrix}
	\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
	\end{bmatrix} \mbox{ or } \begin{bmatrix}
	\cos \theta & \sin \theta \\ -\sin \theta & \cos \theta
	\end{bmatrix}
\end{equation*}\\

(c) Since $Q$ is an orthogonal matrix, for any vector $\textbf{x} \in \mathbb{R}^2$, $\Vert Q\textbf{x} \Vert = \Vert \textbf{x} \Vert$. \\

Now, suppose that the $2 \times 2$ orthogonal matrix $Q$ has the form $Q = \begin{bmatrix}
	\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
\end{bmatrix}$. Let $\textbf{x} = \begin{bmatrix}
	x \\ y
\end{bmatrix}$ be a vector in $\mathbb{R}^2$, then the new vector $Q\textbf{x}$ is given as \begin{equation*}
	Q\textbf{x} = \begin{bmatrix}
		\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
	\end{bmatrix} \begin{bmatrix}
		x \\ y
	\end{bmatrix} = \begin{bmatrix}
		x\cos\theta + y\sin\theta \\ x\sin\theta - y\cos\theta
	\end{bmatrix}
\end{equation*} Then, if the angle between $\textbf{x}$ and $\begin{bmatrix}
\cos(\theta/2) \\ \sin(\theta/2)
\end{bmatrix}$ is $\phi$ ($0 \le \phi \le \pi$), \begin{equation*}
	\cos\phi = \frac{x \cos (\theta / 2) + y \sin (\theta /2)}{\sqrt{x^2 + y^2}}
\end{equation*} And if the angle between $Q\textbf{x}$ and $\begin{bmatrix}
\cos(\theta/2) \\ \sin(\theta/2)
\end{bmatrix}$ is $\phi'$ ($0 \le \phi' \le \pi$), \begin{align*}
	\cos \phi' &= \frac{(x \cos \theta + y \sin \theta)\cos(\theta / 2) + (x \sin \theta - y \cos \theta)\sin(\theta / 2)}{\sqrt{(x \cos \theta + y \sin \theta)^2 + (x \sin \theta - y \cos \theta)^2}} \\
	&= \frac{x[\cos\theta\cos(\theta/2)+\sin\theta\sin(\theta/2)] + y[\sin\theta\cos(\theta/2)-\cos\theta\sin(\theta/2)]}{\sqrt{x^2 + y^2}} \\
	&= \frac{x\cos(\theta/2) + y\sin(\theta/2)}{\sqrt{x^2 + y^2}} = \cos\phi
\end{align*}
Therefore, $Q$ corresponds to a reflection to the line $\textbf{x} = t\begin{bmatrix}
	\cos(\theta / 2) \\ \sin(\theta / 2)
\end{bmatrix}, t \in \mathbb{R}$. \\

(Another proof for reflection)
We already have $\Vert Q\textbf{x} \Vert = \Vert \textbf{x} \Vert$. So our proof can be done by proving $ Q\textbf{x}+\textbf{x} $ is in specific line.

$ Q\textbf{x}+\textbf{x}=\begin{bmatrix}
x(1+\cos{\theta})+y\sin{\theta} \\ x\sin{\theta}+y(1-\cos{\theta})
\end{bmatrix} $. Since $ \dfrac{\sin{\theta}}{1+\cos{\theta}}
=\dfrac{1-\cos{\theta}}{\sin{\theta}}=\tan{\dfrac{\theta}{2}} $, \\
$ \dfrac{x\sin{\theta}}{x(1+\cos{\theta})}
=\dfrac{y(1-\cos{\theta})}{y\sin{\theta}}
=\dfrac{x\sin{\theta}+y(1-\cos{\theta})}{x(1+\cos{\theta})+y\sin{\theta}}=\tan{\dfrac{\theta}{2}}. (\because \text{가비의 리}) $ \\
$ \therefore Q\textbf{x}+\textbf{x} $ is in the line $\textbf{x} = t\begin{bmatrix}
\cos(\theta / 2) \\ \sin(\theta / 2)
\end{bmatrix}, t \in \mathbb{R}$.


Suppose that the $2 \times 2$ orthogonal matrix $Q$ has the form $Q = \begin{bmatrix}
	\cos \theta & \sin \theta \\ -\sin \theta & \cos \theta
\end{bmatrix}$. Let $\textbf{x} = \begin{bmatrix}
	x \\ y
\end{bmatrix}$ be a vector in $\mathbb{R}^2$, then the new vector $Q\textbf{x}$ is given as \begin{equation*}
	Q\textbf{x} = \begin{bmatrix}
		\cos \theta & \sin \theta \\ -\sin \theta & \cos \theta
	\end{bmatrix} \begin{bmatrix}
		x \\ y
	\end{bmatrix} = \begin{bmatrix}
		x \cos \theta + y \sin \theta \\ -x \sin \theta + y \cos \theta
	\end{bmatrix}
\end{equation*}
If the angle between $\textbf{x}$ and $Q\textbf{x}$ is $\phi$, \begin{align*}
	\cos \phi &= \frac{x(x \cos \theta + y \sin \theta) + y(-x \sin \theta + y \cos \theta)}{\sqrt{x^2 + y^2}\sqrt{(x \cos \theta + y \cos \theta)^2 + (-x \sin \theta + y \cos \theta)^2}} \\
	&= \frac{\cos\theta(x^2 + y^2)}{x^2 + y^2} = \cos \theta
\end{align*}
Therefore, $\theta = \phi$ and $Q$ corresponds to a rotation by $\theta$. \\

(d) We have shown that if $Q$ is the form $Q = \begin{bmatrix}
	\cos \theta & \sin \theta \\ -\sin \theta & \cos \theta
\end{bmatrix}$, then $Q$ corresponds to a rotation, and if $Q$ is the form $Q = \begin{bmatrix}
	\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
\end{bmatrix}$, then $Q$ corresponds to a reflection. Since \begin{equation*}
	\begin{vmatrix}
		\cos \theta & \sin \theta \\ -\sin \theta & \cos \theta
	\end{vmatrix} = \cos^2\theta + \sin^2\theta = 1 \mbox{ and } \begin{vmatrix}
		\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
	\end{vmatrix} = -(\cos^2 \theta + \sin^2\theta) = -1
\end{equation*} if $\det Q = 1$ then $Q$ corresponds to a rotation, and if $\det Q = -1$ then $Q$ corresponds to a reflection. \\

\textbf{(5.1 33)} \\
Let $A$ ad $B$ be orthogonal matrices. \\

(a) Since $\inv{A} = A^T$ and $\inv{B} = B^T$, \begin{equation*}
	A(A^T + B^T)B = AA^TB + AB^TB = IB + AI = A + B
\end{equation*}

(b) By (a), $A(A^T + B^T)B = A+B$ so \begin{align*}
	\det{(A(A^T+B^T)B)} &= \det{A}\det{(A^T + B^T)}\det{B} \\
	&= \det{A}\det{(A+B)^T}\det{B}
	= \det{A}\det{(A+B)}\det{B} = \det{(A+B)} \\
	&\therefore \det{(A+B)}(\det{A}\det{B} - 1) = 0
\end{align*}
Note that $\det{A} = \pm 1$ and $\det{B} = \pm 1$ since they are orthogonal matrices. Since $\det A + \det B = 0$, \begin{equation*}
	\det A \det B = (-\det B)\det B = -(\det B)^2 = -1
\end{equation*} Therefore, $\det{(A+B)} = 0$, so $A+B$ is not invertible. \\

\textbf{(5.1 34)} \\
\begin{equation*}
	Q^T = \begin{bmatrix}
		x_1 & \textbf{y}^T \\ \textbf{y} & I^T - (\frac{1}{1 - x_1})(\textbf{y}\textbf{y}^T)^T
	\end{bmatrix} = \begin{bmatrix}
		x_1 & \textbf{y}^T \\ \textbf{y} & I - (\frac{1}{1-x_1})\textbf{y}\textbf{y}^T
	\end{bmatrix} = Q
\end{equation*} Now consider $Q^TQ = Q^2$. \begin{align*}
	Q^TQ = Q^2 &= \begin{bmatrix}
		x_1^2 + \textbf{y}^T\textbf{y} & x_1\textbf{y}^T + \textbf{y}^T(I - (\frac{1}{1-x_1})\textbf{y}\textbf{y}^T) \\
		x_1\textbf{y} + (I - (\frac{1}{1-x_1})\textbf{y}\textbf{y}^T)\textbf{y} & \textbf{y}\textbf{y}^T + (I - (\frac{1}{1-x_1})\textbf{y}\textbf{y}^T)^2
	\end{bmatrix} \\
	&= \begin{bmatrix}
		x_1^2 + \textbf{y}^T\textbf{y} & (1+x_1)\textbf{y}^T - (\frac{1}{1-x_1})\textbf{y}^T\textbf{y}\textbf{y}^T \\
		(1+x_1)\textbf{y} - (\frac{1}{1-x_1})\textbf{y}\textbf{y}^T\textbf{y} & I + (1 - \frac{2}{1-x_1})\textbf{y}\textbf{y}^T + (\frac{1}{1-x_1})^2\textbf{y}\textbf{y}^T\textbf{y}\textbf{y}^T
	\end{bmatrix}
\end{align*}
Since $\textbf{y}^T\textbf{y} = x_2^2 + \cdots + x_n^2 = 1 - x_1^2$, \begin{align*}
	Q^TQ = Q^2 &= \begin{bmatrix}
		x_1^2 + 1 - x_1^2 & (1 + x_1)\textbf{y}^T - (1 + x_1)\textbf{y}^T \\
		(1 + x_1)\textbf{y} - (1 + x_1)\textbf{y} & 
		I - (\frac{1+x_1}{1-x_1})\textbf{y}\textbf{y}^T + (\frac{1+x_1}{1-x_1})\textbf{y}\textbf{y}^T
	\end{bmatrix} \\
	&= \begin{bmatrix}
		1 & \textbf{0} \\ \textbf{0} & I
	\end{bmatrix} = I
\end{align*}
Therefore, $Q$ is orthogonal. \\

\textbf{(5.1 35)} \\
\textit{Solution 1.} \\
We prove this with mathematical induction on each columns of $A = [a_{ij}]$. \\

Let $A = \begin{bmatrix}
	\textbf{a}_1 & \textbf{a}_2  & \cdots & \textbf{a}_n
\end{bmatrix}$ be an \nbyn matrix. \\

(i) In the first column of $A$, $a_{21} = \cdots = a_{n1} = 0$ since $A$ is upper triangular. \\

(ii) Suppose that for $i = 1, 2, \cdots, k$ $(k < n)$, $a_{ji} = 0$ if $i \neq j$, and $a_{ii} \neq 0$. \\

 Since $A$ is orthogonal, for $i = 1, 2, \cdots, k$, \begin{equation*}
	\textbf{a}_i \cdot \textbf{a}_{k+1} = \sum_{j=1}^{n}a_{ji}a_{j(k+1)} = a_{ii}a_{i(k+1)} = 0
\end{equation*} so $a_{1(k+1)} = \cdots = a_{k(k+1)} = 0$. Since $A$ is upper triangular, $a_{(k+2)(k+1)} = \cdots = a_{n(k+1)} = 0$. Since $\textbf{a}_{k+1}$ is a unit vector, $a_{(k+1)(k+1)} \neq 0$ so the induction proposition holds for $(k+1)$th column. \\

(i) and (ii) give that $A$ is a diagonal matrix. \\

\textit{Solution 2.} \\
If $\lambda$ is an eigenvalue of $A$ with corresponding eigenvector $\textbf{v}$, $A$ is orthogonal so \begin{equation*}
	\Vert Q\textbf{x} \Vert = \Vert \lambda \textbf{x} \Vert = \vert \lambda \vert \Vert \textbf{x} \Vert = \Vert \textbf{x} \Vert
\end{equation*} thus $\vert \lambda \vert = 1$. Since the diagonal entries of $A$ are the eigenvalues of $A$, the diagonal entries should be either 1 or -1. Since the norm of each columns of $A$ are 1, the non-diagonal entries of $A$ are zero. Therefore, $A$ is diagonal matrix. \\

\textit{Solution 3.} Since $\inv{A} = A^T$ and $\inv{A}$ is also an upper triangular matrix (이 부분은 추가 증명이 필요.), $A$ is diagonal matrix. \\

\textbf{(5.1 36)} \\
Let $A$ be an $m \times n$ matrix. Since $n > m$, rank($A$) $\le m < n$ so there exists a nonzero vector $\textbf{x} \in \mathbb{R}^n$ which is in null($A$). That is, $A\textbf{x} = \textbf{0}$ so \begin{equation*}
	\Vert A\textbf{x} \Vert = \Vert \textbf{0} \Vert = 0 \neq \Vert \textbf{x} \Vert
\end{equation*}
Therefore, an $m \times n$ matrix which satisfies $\Vert A\textbf{x} \Vert = \Vert \textbf{x} \Vert$ for all $\textbf{x} \in \mathbb{R}^n$ cannot exist if $m < n$. \\

\textbf{(5.1 37)} \\
(a) Let $\mathcal{B} = \{ \textbf{v}_1, \cdots, \textbf{v}_n \}$ be an orthonormal basis for \Rn. Let $\textbf{x}, \textbf{y} \in \Rnn$ then there exist scalars $c_1, \cdots, c_n$ and $d_1, \cdots, d_n$ such that \begin{equation*}
	\textbf{x} = c_1\textbf{v}_1 + \cdots + c_n\textbf{v}_n \mbox { and } \textbf{y} = d_1\textbf{v}_1 + \cdots + d_n\textbf{v}_n
\end{equation*}
So the dot product of $\textbf{x}$ and $\textbf{y}$ is given as \begin{align*}
	\textbf{x} \cdot \textbf{y} &= (c_1\textbf{v}_1 + \cdots + c_n\textbf{v}_n) \cdot (d_1\textbf{v}_1 + \cdots + d_n\textbf{v}_n) \\
	&= \sum_{i=1}^{n}\sum_{j=1}^{n}c_id_j(\textbf{v}_i \cdot \textbf{v}_j) \\
	&= \sum_{i=1}^{n}c_id_i(\textbf{v}_i \cdot \textbf{v}_i)
	= \sum_{i=1}^{n}((c_i\textbf{v}_i) \cdot (d_i\textbf{v}_i)) = \sum_{i=1}^{n}c_id_i \\
	&= \sum_{i=1}^{n}(\textbf{x} \cdot \textbf{v}_i)(\textbf{y} \cdot \textbf{v}_i)
\end{align*} since $\textbf{v}_1, \cdots, \textbf{v}_n$ are unit vectors and are orthogonal. \\

(b) Since $\textbf{x} \cdot \textbf{v}_i$ and $\textbf{y} \cdot \textbf{v}_i$ are the $i$th components of the coordinate vector $[\textbf{x}]_{\mathcal{B}}$ and $[\textbf{y}]_{\mathcal{B}}$, \begin{equation*}
	\textbf{x} \cdot \textbf{y} = [\textbf{x}]_{\mathcal{B}}\cdot[\textbf{y}]_{\mathcal{B}}
\end{equation*}

\textbf{(5.2 25)} \\
Counterexample : Let $W = \mathbb{R}^2$ and $\textbf{v} = \begin{bmatrix}
	1 \\ 1
\end{bmatrix}$. Let $\textbf{w} = \begin{bmatrix}
	1 \\ 0
\end{bmatrix}$ and $\textbf{w}' = \begin{bmatrix}
	0 \\ 1
\end{bmatrix}$ then $\textbf{w}$ is a vector of $\mathbb{R}^2$ which is in $W$ and orthogonal to $\textbf{w}'$, and $\textbf{v} = \textbf{w} + \textbf{w}'$. However, since $W' = \{ \textbf{0} \}$, $\textbf{w}' \notin W^\perp$. \\

\textbf{(5.2 26)} \\
Let $\{ \textbf{v}_1, \cdots, \textbf{v}_n \}$ be an orthogonal basis for $\mathbb{R}^n$ and let $W$ = span($\textbf{v}_1, \cdots, \textbf{v}_k$). \\

(i) For any vector $\textbf{u} \in W^\perp$, there exist scalars $c_1, \cdots, c_n$ such that \begin{equation*}
	\textbf{u} = c_1\textbf{v}_1 + \cdots + c_n\textbf{v}_n
\end{equation*} Since $\textbf{u}$ is orthogonal to all vectors in $W$, for $i = 1, 2, \cdots, k$, \begin{equation*}
	\textbf{u} \cdot \textbf{v}_i = (c_1\textbf{v}_1 + \cdots + c_n\textbf{v}_n) \cdot \textbf{v}_i = c_i(\textbf{v}_i \cdot \textbf{v}_i) = 0
\end{equation*} Since $\textbf{v}_i \neq \textbf{0}$, $c_1 = c_2 = \cdots = c_k = 0$. Therefore, $\textbf{u} = c_{k+1}\textbf{v}_{k+1} + \cdots + c_n\textbf{v}_n$ so $\textbf{u} \in $ span($\textbf{v}_{k+1}, \cdots, \textbf{v}_n$). Thus, $W^\perp \subset$ span($\textbf{v}_{k+1}, \cdots, \textbf{v}_n$). \\

(ii) Since $\textbf{v}_1, \cdots, \textbf{v}_k$ are orthogonal, $\{ \textbf{v}_1, \cdots, \textbf{v}_k \}$ forms a basis for $W$. For any vector $\textbf{u} \in$ span($\textbf{v}_{k+1}, \cdots, \textbf{v}_n$), there exist scalars $c_{k+1}, \cdots, c_n$ such that \begin{equation*}
	\textbf{u} = c_{k+1}\textbf{v}_{k+1} + \cdots + c_n\textbf{v}_n
\end{equation*} so \begin{equation*}
	\textbf{u} \cdot \textbf{v}_i = (c_{k+1}\textbf{v}_{k+1} + \cdots + c_n\textbf{v}_n) \cdot \textbf{v}_i = 0
\end{equation*} for $i = 1, 2, \cdots, k$. Therefore, by Theorem 5.9(d), $\textbf{u} \in W^\perp$. Thus, span($\textbf{v}_{k+1}, \cdots, \textbf{v}_n$) $\subset W^\perp$. \\

By (i) and (ii), $W^\perp$ = span($\textbf{v}_{k+1}, \cdots, \textbf{v}_n$). \\

\textit{Note.} 6장 개념 이용시, vector space $V, W$에 대해 $V \subset W, W \subset V, \dim V = \dim W$ 중 2개가 성립하면 나머지 하나도 성립하므로 (i), (ii) 중 하나만 증명해도 무방하다. \\

\textbf{(5.2 27 + 5.2 28) : Projection to a Subspace} \\
Note that by the Orthogonal Decomposition Theorem, $\textbf{w}$ = proj$_W(\textbf{x})$ and $\textbf{w}^\perp$ = perp$_W(\textbf{x})$ are the unique vectors satisfying $\textbf{w} \in W, \textbf{w}^\perp \in W^\perp$, and $\textbf{w} + \textbf{w}^\perp = \textbf{x}$. \\

\textbf{27}
($\Rightarrow$) Suppose that $\textbf{x} \in W$. Then $\textbf{x} = \textbf{x} + \textbf{0}$ is the unique orthogonal decomposition of $\textbf{x}$ with respect to $W$, since $\textbf{x} \in W$ and $\textbf{0} \in W^\perp$. Therefore, $\textbf{x}$ = proj$_W(\textbf{x})$. \\

($\Leftarrow$) Since proj$_W(\textbf{x}) \in W$, $\textbf{x}$ is in $W$. \\

\textbf{28}
($\Rightarrow$) Suppose that $\textbf{x}$ is orthogonal to $W$, then $\textbf{x} \in W^\perp$. Then $\textbf{x} = \textbf{0} + \textbf{x}$ is the unique orthogonal decomposition of $\textbf{x}$ with respect to $W$, since $\textbf{0} \in W$ and $\textbf{x} \in W^\perp$. Therefore, proj$_W(\textbf{x}) = \textbf{0}$. \\

($\Leftarrow$) Since perp$_W\textbf{x} \in W^\perp$, $\textbf{x} = \textbf{x} - $proj$_W(\textbf{x})$ = perp$_W(\textbf{x})$ is in $W^\perp$. Therefore, $\textbf{x}$ is orthogonal to $W$. \\

\textbf{(5.2 29)} \\
For all $\textbf{x} \in \Rnn$, proj$_W(\textbf{x}) \in W$ by Exercise 5.2 27. Therefore, proj$_W$(proj$_W(\textbf{x})$) = proj$_W(\textbf{x})$. \\

\textbf{(5.2 30)} \\
\textit{Solution 1.} \\
Let $S = \{ \textbf{v}_1, \cdots, \textbf{v}_k \}$ be an orthonormal set in \Rn, and let $\textbf{x}$ be a vector in \Rn. Let $W = \textnormal{span}(S)$, where $W$ is a subspace of \Rn, then $S$ is a basis for $W$. \\

(a) By the Orthogonal Decomposition Theorem, \begin{align*}
	\Vert \textbf{x} \Vert ^ 2 &= \textbf{x} \cdot \textbf{x} \\
	&= (\textnormal{proj}_W(\textbf{x}) + \textnormal{perp}_W(\textbf{x})) \cdot  (\textnormal{proj}_W(\textbf{x}) + \textnormal{perp}_W(\textbf{x})) \\
	&= \Vert \textnormal{proj}_W(\textbf{x}) \Vert^2 + \Vert \textnormal{perp}_W(\textbf{x}) \Vert^2 \ge \Vert \textnormal{proj}_W(\textbf{x}) \Vert^2
\end{align*}\\
The definition of proj$_W(\textbf{x})$ gives that \begin{align*}
	\Vert \textnormal{proj}_W(\textbf{x}) \Vert ^ 2 &= \textnormal{proj}_W(\textbf{x}) \cdot\textnormal{proj}_W(\textbf{x}) \\
	&= \left( \left (\frac{\textbf{x} \cdot \textbf{v}_1}{\textbf{v}_1 \cdot \textbf{v}_1} \right)\textbf{v}_1 + \cdots + \left (\frac{\textbf{x} \cdot \textbf{v}_k}{\textbf{v}_k \cdot \textbf{v}_k} \right)\textbf{v}_k \right) \cdot \left( \left (\frac{\textbf{x} \cdot \textbf{v}_1}{\textbf{v}_1 \cdot \textbf{v}_1} \right)\textbf{v}_1 + \cdots + \left (\frac{\textbf{x} \cdot \textbf{v}_k}{\textbf{v}_k \cdot \textbf{v}_k} \right)\textbf{v}_k \right) \\
	&= \left( (\textbf{x} \cdot \textbf{v}_1)\textbf{v}_1 + \cdots + (\textbf{x} \cdot \textbf{v}_k)\textbf{v}_k \right) \cdot \left( (\textbf{x} \cdot \textbf{v}_1)\textbf{v}_1 + \cdots + (\textbf{x} \cdot \textbf{v}_k)\textbf{v}_k \right) \\
	&= \vert \textbf{x} \cdot \textbf{v}_i \vert^2 (\textbf{v}_1 \cdot \textbf{v}_1) + \cdots + \vert \textbf{x} \cdot \textbf{v}_k \vert^2 (\textbf{v}_k \cdot \textbf{v}_k) \\
	&= \vert \textbf{x} \cdot \textbf{v}_1 \vert ^ 2 + \cdots + \vert \textbf{x} \cdot \textbf{v}_k \vert ^2
\end{align*} Since $S$ is an orthonormal set of vectors. Therefore, \begin{equation*}
	\Vert \textbf{x} \Vert ^ 2 \ge \vert \textbf{x} \cdot \textbf{v}_1 \vert ^ 2 + \cdots + \vert \textbf{x} \cdot \textbf{v}_k \vert ^2
\end{equation*}

(b) The proof of (a) gives \begin{equation*}
	\Vert \textbf{x} \Vert ^ 2 \ge \Vert \textnormal{proj}_W(\textbf{x}) \Vert^2 = \vert \textbf{x} \cdot \textbf{v}_1 \vert ^ 2 + \cdots + \vert \textbf{x} \cdot \textbf{v}_k \vert ^2
\end{equation*} so the equality holds if and only if $\textbf{x} = \textnormal{proj}_W(\textbf{x})$. Therefore, by Exercise 5.2 27, the equality holds if and only if $\textbf{x} \in W = \textnormal{span}(S)$.\\

\textit{Solution 2.} \\
(a) Using the Gram-Schmidt Process, we can extend $S$ to an orthonormal basis $\{ \textbf{v}_1, \cdots, \textbf{v}_n \}$ of $\mathbb{R}^n$. Then for any vector $\textbf{x} \in \mathbb{R}^n$, \begin{align*}
	\Vert \textbf{x} \Vert ^ 2 &= \textbf{x} \cdot \textbf{x} \\
	&= \vert \textbf{x} \cdot \textbf{v}_1 \vert ^ 2 + \cdots + \vert \textbf{x} \cdot \textbf{v}_n \vert ^2 \\
	&\ge \vert \textbf{x} \cdot \textbf{v}_1 \vert ^ 2 + \cdots + \vert \textbf{x} \cdot \textbf{v}_k \vert ^2
\end{align*} \\

(b) The equality holds when $\textbf{x} \cdot \textbf{v}_{k+1} = \cdots = \textbf{x} \cdot \textbf{v}_n = 0$. If then, $\textbf{x} \in \textnormal{span}(S)$. \\

\textbf{(5.3 19)} \\
A $QR$ factorization of $A$ is $A = AI$, since $A$ is an orthogonal matrix, and $I$ is an invertible upper triangular matrix. \\

\textbf{(5.3 20)} \\
\begin{table}[H]
	\begin{center}
		\begin{tabular}{c}
			$A$ is invertible. \\
			\\
			$\Updownarrow$ (F.T.I.M) \\
			\\
			Columns of $A$ are linearly independent. \\
			\\
			$\Updownarrow$ (Theorem 5.16) \\
			\\
			$A$ can be factorized as $A = QR$, where $Q$ is an orthogonal matrix \\ and $R$ is an invertible upper triangular matrix. \\
			\\
			$\Updownarrow$ (F.T.I.M) \\
			\\
			$R$ has nonzero diagonal entries, since the eigenvalues of $R$ are not zero.
		\end{tabular}
	\end{center}
\end{table}

\textbf{(5.3 23)} \\
Let $A$ be an $m \times n$ matrix with linearly independent columns. Let $Q$ be an $m \times n$ matrix with orthonormal columns, and $R$ an $n \times n$ upper triangular matrix such that $A = QR$. \\

Suppose that $R$ is not invertible, then by F.T.I.M, there exists a nonzero vector $\textbf{x} \in \Rnn$ such that $R\textbf{x} = \textbf{0}$. Since $A\textbf{x} = QR\textbf{x} = \textbf{0}$, there exists a nontrivial solution of $A\textbf{x} = \textbf{0}$, thus the columns of $A$ cannot be linearly independent. Therefore, $R$ is invertible. \\

\textbf{(5.3 24)} \\
Let $A$ be an $m \times n$ matrix with linearly independent columns, which can be factorized as $A = QR$ where $Q$ is an $m \times n$ matrix with orthonormal columns of $R$ is an $n \times n$ invertible upper triangular matrix. \\

Since $A = QR$, the columns of $A$ can be expressed as a linear combination of the columns of $Q$. (Remind Exercise 3.1 26.) Also, since $Q = A\inv{R}$, the columns of $Q$ can be expressed as a linear combination of the columns of $A$. Therefore, col($A$) = col($Q$). \\

\textbf{(5.4 13)} \\
The Spectral Theorem states that a matrix is orthogonally diagonalizable if and only if it is symmetric. Since $A$ and $B$ are orthogonally diagonalizable, $A$ and $B$ are symmetric. \\

(a) Since $A + B$ is symmetric, $A+B$ is orthogonally diagonalizable. \\

(b) For any scalar $c$, $cA$ is symmetric, so $cA$ is orthogonally diagonalizable. \\

(c) Since $A^2$ is symmetric, $A^2$ is orthogonally diagonalizable. \\

\textbf{(5.4 14)} \\
Let $A$ be an orthogonally diagonalizable invertible matrix, then $A$ is symmetric by the Spectral Theorem. Since $(\inv{A})^T = \inv{(A^T)} = \inv{A}$, $\inv{A}$ is also symmetric and $\inv{A}$ is orthogonally diagonalizable. \\

\textbf{(5.4 15)} \\
Let $A$ and $B$ be orthogonally diagonalizable matrices, then $A$ and $B$ are symmetric by the Spectral Theorem. Since $AB = BA$, \begin{equation*}
	(AB)^T = (BA)^T = A^TB^T = AB
\end{equation*} $AB$ is also symmetric, so $AB$ is orthogonally diagonalizable. \\

\textbf{(5.4 16)} \\
Since $A$ is symmetric, by the Spectral Theorem, $A$ is orthogonally diagonalizable so there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that $A = QDQ^T$. \\

($\Rightarrow$) If every eigenvalues of $A$ are nonnegative, the diagonal entries of $D$ are nonnegative so there exists a diagonal matrix $D_1$ such that $D_1^2 = D$. (We take the square roots of the entries of $D$ for each entries of $D_1$.) Let $B$ = $QD_1Q^T$, then\begin{equation*}
	A = QDQ^T = QD_1^2Q^T = QD_1Q^TQD_1Q^T = B^2
\end{equation*} Since $B$ is orthogonally diagonalizable, $B$ is symmetric. \\

($\Leftarrow$) Suppose that there exists a symmetric matrix $B$ such that $A = B^2$. By the Spectral Theorem, $B$ is orthogonally diagonalizable so there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that $B = QDQ^T$. Then \begin{equation*}
	A = B^2 = (QDQ^T)(QDQ^T) = QD^2Q^T
\end{equation*} and $D^2$ is also a diagonal matrix with nonnegative diagonal entries. Since $A \sim D^2$, the eigenvalues of $A$ are the eigenvalues of $D^2$, so they are nonnegative. \\

\textbf{(5.4 25)} \\
For any vector $\textbf{v} \in \Rnn$, \begin{equation*}
	(\textbf{q}\textbf{q}^T)\textbf{v} = \textbf{q}(\textbf{q}^T\textbf{v}) = (\textbf{q} \cdot \textbf{v})\textbf{q} = \left (\frac{\textbf{q} \cdot \textbf{v}}{\textbf{q} \cdot \textbf{q}} \right )\textbf{q} = \textnormal{proj}_W(\textbf{v})
\end{equation*}

\textbf{(5.4 26)} \\
Let $\{ \textbf{q}_1, \cdots, \textbf{q}_k \}$ be an orthonormal basis of $W$. \\

(a) For any vector $\textbf{v} \in \Rnn$, \begin{align*}
	P\textbf{v} &= (\textbf{q}_1\textbf{q}_1^T + \cdots + \textbf{q}_k\textbf{q}_k^T)\textbf{v} \\
	&= \textbf{q}_1\textbf{q}_1^T\textbf{v} + \cdots + \textbf{q}_k\textbf{q}_k^T\textbf{v} \\
	&= (\textbf{q}_1 \cdot \textbf{v})\textbf{q}_1 + \cdots + (\textbf{q}_k \cdot \textbf{v})\textbf{q}_k \\
	&= (\frac{\textbf{q}_1 \cdot \textbf{v}}{\textbf{q}_1 \cdot \textbf{q}_1})\textbf{q}_1 + \cdots + (\frac{\textbf{q}_k \cdot \textbf{v}}{\textbf{q}_k \cdot \textbf{q}_k})\textbf{q}_k = \textnormal{proj}_W(\textbf{v})
\end{align*}
(b) Since $(\textbf{q}_i\textbf{q}_i^T)^T = \textbf{q}_i\textbf{q}_i^T$ for $\textbf{q}_1, \cdots, \textbf{q}_k$, $P = \textbf{q}_1\textbf{q}_1^T + \cdots + \textbf{q}_k\textbf{q}_k^T$ is symmetric. Also, for any vector $\textbf{v} \in \Rnn$, \begin{equation*}
	P^2\textbf{v} = \textnormal{proj}_W(\textnormal{proj}_W(\textbf{v})) = \textnormal{proj}_W(\textbf{v}) = P\textbf{v}
\end{equation*} Since $(P^2 - P)\textbf{v} = \textbf{0}$ for any vector $\textbf{v} \in \Rnn$, $P^2  - P = O$. \\

(c) Let $Q = \begin{bmatrix}
	\textbf{q}_1 & \cdots & \textbf{q}_k
\end{bmatrix}$, then \begin{align*}
	QQ^T = \begin{bmatrix}
		\textbf{q}_1 & \cdots & \textbf{q}_k
	\end{bmatrix} \begin{bmatrix}
		\textbf{q}_1^T \\ \vdots \\ \textbf{q}_k^T
	\end{bmatrix} = \textbf{q}_1\textbf{q}_1^T + \cdots + \textbf{q}_k\textbf{q}_k^T = P
\end{align*}
Since $P = QQ^T$, rank($P$) = rank($QQ^T$) $\le$ rank($Q$) = $k$. (Remind Exercise 3.5 60a) Also, for $i = 1, 2, \cdots, k$, \begin{align*}
	P\textbf{q}_i &= (\textbf{q}_1\textbf{q}_1^T + \cdots + \textbf{q}_k\textbf{q}_k^T)\textbf{q}_i \\
	&= (\textbf{q}_1 \cdot \textbf{q}_i)\textbf{q}_1 + \cdots + (\textbf{q}_k \cdot \textbf{q}_i)\textbf{q}_k = (\textbf{q}_i \cdot \textbf{q}_i) \textbf{q}_i = \textbf{q}_i
\end{align*} Thus, $\textbf{q}_i = P\textbf{q}_i \in \textnormal{col}(P)$. Since $\textbf{q}_1, \cdots, \textbf{q}_k$ are linearly independent, rank($P$) $\ge k$. \\

Therefore, rank($P$) = $k$. \\

\textbf{(5.4 27)} \\
We prove the proposition by mathematical induction. \\

(i) If $A$ is a $1 \times 1$ real matrix, then $A = I^TAI$, where $A$ is an upper triangular matrix and $I$ is an orthogonal matrix. Therefore, the proposition holds. \\

(ii) Suppose that for any real $(n-1) \times (n-1)$ matrix $A$ with real eigenvalues, there exist an $(n-1) \times (n-1)$ orthogonal matrix $Q$ and an  $(n-1) \times (n-1)$ upper triangular matrix $T$ such that $Q^TAQ = T$. \\

Let $A$ be an $n \times n$ real matrix with real eigenvalues. Let $\lambda_1$ be an eigenvalue of $A$, then there exists a real unit eigenvector $\textbf{v}_1$ corresponding to $\lambda_1$. Since we can extend $\textbf{v}_1$ to an orthonormal basis $\{ \textbf{v}_1, \cdots, \textbf{v}_n \}
$ of $\mathbb{R}^n$ using the Gram-Schmidt Process, let $Q_1$ be an orthogonal matrix such that \begin{equation*}
	Q_1 = \begin{bmatrix}
		\textbf{v}_1 & \cdots & \textbf{v}_n
	\end{bmatrix}
\end{equation*} Since $\textbf{v}_1, \cdots, \textbf{v}_n$ are orthonormal, \begin{align*}
	Q_1^TAQ_1 &= \begin{bmatrix}
		\textbf{v}_1^T \\ \vdots \\ \textbf{v}_n^T
	\end{bmatrix}A\begin{bmatrix}
		\textbf{v}_1 & \textbf{v}_2 & \cdots & \textbf{v}_n
	\end{bmatrix} \\
	&= \begin{bmatrix}
		\textbf{v}_1^T \\ \vdots \\ \textbf{v}_n^T
	\end{bmatrix}\begin{bmatrix}
		A\textbf{v}_1 & A\textbf{v}_2 &  \cdots & A\textbf{v}_n
	\end{bmatrix} \\
	&= \begin{bmatrix}
		\textbf{v}_1^T \\ \vdots \\ \textbf{v}_n^T
	\end{bmatrix} \begin{bmatrix}
		\lambda_1\textbf{v}_1 & A\textbf{v}_2 & \cdots & A\textbf{v}_n
	\end{bmatrix} \\
	&= \begin{bmatrix}
		\lambda_1 & ? \\
		\textbf{0} & A_1
	\end{bmatrix} = B
\end{align*} as $\textbf{v}_1^T(\lambda_1\textbf{v}_1) = \lambda_1(\textbf{v}_1 \cdot \textbf{v}_1) = \lambda_1$ and $\textbf{v}_i^T(\lambda_1\textbf{v}_1) = \lambda_1(\textbf{v}_i \cdot \textbf{v}_1) = 0$ if $i \neq 1$. Since $A \sim B$, the characteristic polynomial of $B$ is equal to that of $A$. Since $c_A(\lambda) = c_B(\lambda) = (\lambda - \lambda_1)c_{A_1}(\lambda)$ (Exercise 4.3 39), the eigenvalues of $A_1$ are also eigenvalues of $A$, so they are real. Therefore, $A_1$ is an $(n-1) \times (n-1)$ matrix with real eigenvalues. \\

By inductive hypothesis, there exist an $(n-1) \times (n-1)$ orthogonal matrix $P_2$ and an $(n-1) \times (n-1)$ upper triangular matrix $T_1$ such that $P_2^TA_1P_2 = T_1$. Let $Q_2$ be \begin{equation*}
	Q_2 = \begin{bmatrix}
		1 & \textbf{0} \\ \textbf{0} & P_2
	\end{bmatrix}
\end{equation*} Then \begin{align*}
	(Q_1Q_2)^TA(Q_1Q_2) &= Q_2^TQ_1^TAQ_1Q_2 = Q_2^TBQ_2 \\
	&= \begin{bmatrix}
		1 & \textbf{0} \\ \textbf{0} & P_2^T
	\end{bmatrix}\begin{bmatrix}
		\lambda_1 & ? \\ \textbf{0} & A_1
	\end{bmatrix}\begin{bmatrix}
		1 & \textbf{0} \\ \textbf{0} & P_2
	\end{bmatrix} \\
	&= \begin{bmatrix}
		\lambda_1 & ? \\ \textbf{0} & P_2^TA_1P_2
	\end{bmatrix} = \begin{bmatrix}
		\lambda_1 & ? \\ \textbf{0} & T_1
	\end{bmatrix}
\end{align*} which is an upper triangular matrix. \\

By (i) and (ii), for any size of real matrix $A$, there exist an orthogonal matrix $Q$ and an upper triangular matrix $T$ such that $Q^TAQ = T$. \\

\textbf{(5.4 28)} \\
By Exericse 5.4 27, there exist an orthogonal matrix $Q$ and an upper triangular matrix $T$ such that $Q^TAQ = T$. Note that $\lambda = 0$ is the only eigenvalue of $A$ since $A$ is nilpotent. (Exercise 4.3 20) Since $A \sim T$, $\lambda = 0$ is also the only eigenvalue of $T$, so the diagonal entries of $T$ should be zero.
\fi