\chapter{Vectors}

% \iffalse 부터 \fi 까지는 주석처리됨
\iffalse
\begin{theorem}[Algebraic Properties of Vectors in \Rn] \label{properties_vectors}
	Let $\vec{u}$, $\vec{v}$, $\vec{w}$ be vectors in \Rn and let $c$ and $d$ be scalars. Then
	\begin{enumerate}
		\item $\vec{u}+\vec{v}=\vec{v}+\vec{u}$ : Commutativity
		\item $(\vec{u}+\vec{v})+\vec{w}=\vec{u}+(\vec{v}+\vec{w})$ : Associativity
		\item $\vec{u}+\vec{0}=\vec{u}$
		\item $\vec{u}+(-\vec{u})=\vec{0}$
		\item $c(\vec{u}+\vec{v})=c\vec{u}+c\vec{v}$ : Distributivity
		\item $(c+d)\vec{u}=c\vec{u}+d\vec{u}$ : Distributivity
		\item $c(d\vec{u})=(cd)\vec{u}$
		\item $1\vec{u}=\vec{u}$
	\end{enumerate}
\end{theorem}

A theorem shown above is thm.\ref{properties_vectors}.

\begin{theorem}
	Let $\vec{u}$, $\vec{v}$, and $\vec{w}$ be vectors in $\mathbb{R}^{n}$ and let $c$ be a scalar. Then
	\begin{enumerate}
		\item $\vec{u}\cdot\vec{v}=\vec{v}\cdot\vec{u}$
		\item $\vec{u}\cdot(\vec{v}+\vec{w}) = \vec{u}\cdot\vec{v}+\vec{u}\cdot\vec{w}$
		\item $(c\vec{u})\cdot\vec{v}=c(\vec{u}\cdot\vec{v})$
		\item $\vec{u}\cdot\vec{u}\geq 0$ and $\vec{u}\cdot\vec{u}=0$ iff $\vec{u}=\vec{0}$
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{equation} \label{pi}
	\pi = 3.141592...
	\end{equation}
	By \eqref{pi}, proved! \qedhere
\end{proof}
\fi

\section{Terminology relating to vectors}

A vector can be represented either in geometric way or algebraic way. In geometric definition of vectors, a vector is a \textbf{directed line segment}.
A vector from point $A$ (\textbf{initial point}, or \textbf{tail}) to point $B$ (\textbf{terminal point}, or \textbf{head}) is denoted as $\overrightarrow{AB}$.
Vectors with their tails in the origin is called \textbf{position vectors}, and they are at \textbf{standard position}.
\\
In algebraic view of vectors, a vector is an \textbf{ordered pair} of \textbf{components}. We denote the set of all vectors containing $n$ components in $\mathbb{R}$ as \Rn. Similarly, set of all vectors containing $n$ integer components is $\mathbb{Z}^{n}$.
\\
A vector is written in form of \textbf{column vectors} and \textbf{row vectors}. We use square brackets for denoting vectors, such as 
\begin{displaymath}
\textbf{v} = \begin{bmatrix}4 \\ 1 \\ 6\end{bmatrix},
\begin{bmatrix} 4, & 1, & 6 \end{bmatrix}
\end{displaymath}
\\
A \textbf{zero vector} is a vector which components are all zero. A zero vector is denoted as $\textbf{0}$.
\\
Two vectors are equal if and only if all the components of two vectors are equal. (Of course, the number of components should be same.)
\\
\textbf{Standard unit Vectors} have components which one of them is 1 and rest of them are all 0. Unit vector which has 1 in $i$th component is denoted as $\textbf{e}_i$, and
\begin{displaymath}
\textbf{e}_i = \begin{bmatrix} 0, & 0, & \cdots & 1, & \cdots & 0 \end{bmatrix}
\end{displaymath}
* \textit{Note} We should always denote vector either with arrows ($\vec{v}$), or with boldface letters ($\textbf{v}$). Scalar denotations ($v$) are not allowed.
\\
A set of vectors with $n$ components taken from finite set of integers $\mathbb{Z}_m = \{0, 1, 2,\cdots,m-1\}$ is denoted as $\mathbb{Z}^{n}_{m}$.
$\mathbb{Z}^{n}_{m}$ is closed with respect to operations of vector addition and scalar multiplication (which is defined later). We can perform those operations in same way, but with modulo operations.
Vectors in $\mathbb{Z}^{n}_2$ (all components are 0 or 1) are called \textbf{binary vectors}.

\section{Basic Operations of Vectors}

\textit{Definition.}
Let $\textbf{u}, \textbf{v} \in$ \Rn be $\textbf{u}=\begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix},
\textbf{v}=\begin{bmatrix} v_1, & v_2, & \cdots & v_n \end{bmatrix}.$
Then their \textbf{sum} $\textbf{u}+\textbf{v}$ is defined as
\begin{displaymath}
\textbf{u}+\textbf{v}=\begin{bmatrix}u_1+v_1,&u_2+v_2,&\cdots&u_n+v_n\end{bmatrix}
\end{displaymath}
\\
\textit{Definition.}
Let $\textbf{v} \in$ \Rn be $\textbf{v}=\begin{bmatrix} v_1&v_2&\cdots&v_n \end{bmatrix}$, and let $c$ be a real number. Then their \textbf{scalar multiple} $c\textbf{v}$ is denifed as
\begin{displaymath}
c\textbf{v}=\begin{bmatrix} cv_1, & cv_2, & \cdots & cv_n \end{bmatrix}
\end{displaymath}
\\
\textit{Definition.}
A \textbf{negative} of $\textbf{v}$ is defined as $-\textbf{v}=(-1)\textbf{v}$.
\\\\
\textit{Definition.}
The \textbf{difference} of vectors is defined as $\textbf{u}-\textbf{v}=\textbf{u}+(-\textbf{v})$.
\\\\
\textit{Definition.}
Two vectors are \textbf{parallel} if and only if one vector is a scalar multiple of another. (Thus, zero vector is parallel with all vectors.)

\begin{theorem}[Algebraic Properties of Vectors : Basic Vector Operations]
Let \textbf{u},\textbf{v}, and \textbf{w} be vectors in \Rn and let $c$ and $d$ be scalars.
    \begin{enumerate}
        \item $\textbf{u} + \textbf{v} = \textbf{v} + \textbf{u}$ (Commutative Property of Vector Addition)
        \item $(\textbf{u} + \textbf{v}) + \textbf{u} = \textbf{u} + (\textbf{v} + \textbf{w})$ (Associative Property of Vector Addition)
        \item $\textbf{u} + \textbf{0} = \textbf{u}$
        \item $\textbf{u} + (-\textbf{u}) = \textbf{0}$
        \item $c(\textbf{u} + \textbf{v}) = c\textbf{u} + c\textbf{v}$ (Left-Distributive Property of Scalar Multiplication over Vector Addition)
        \item $(c+d)\textbf{u} = c\textbf{u} + d\textbf{u}$ (Right-Distributive Property of Scalar Multiplication over Vector Addition)
        \item $c(d\textbf{u}) = (cd)\textbf{u}$
        \item $1\textbf{u} = \textbf{u}$
    \end{enumerate}
\end{theorem}

\begin{proof}
Let $\textbf{u}, \textbf{v}, \textbf{w} \in$ \Rn be
$\textbf{u} = \begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix},
 \textbf{v} = \begin{bmatrix} v_1, & v_2, & \cdots & v_n \end{bmatrix},
 \textbf{w} = \begin{bmatrix} w_1, & w_2, & \cdots & w_n \end{bmatrix}$,
 and let $c$, $d$ be scalars.
 
    \begin{enumerate}
        \item
            \begin{align*}
                \textbf{u}+\textbf{v}
                &= \begin{bmatrix} u_1+v_1, & u_2+v_2, & \cdots & u_n+v_n \end{bmatrix} \\
                &= \begin{bmatrix} v_1+u_1, & v_2+u_2, & \cdots & v_n+u_n \end{bmatrix} = \textbf{v} + \textbf{u}
            \end{align*}
        
        \item
            \begin{align*}
                (\textbf{u}+\textbf{v})+\textbf{w}
                &= \begin{bmatrix} u_1+v_1, & u_2+v_2, & \cdots & u_n+v_n \end{bmatrix} +
                \begin{bmatrix} w_1, & w_2, & \cdots & w_n \end{bmatrix} \\
                &= \begin{bmatrix} (u_1+v_1)+w_1, & (u_2+v_2)+w_2, & \cdots & (u_n+v_n)+w_n \end{bmatrix} \\
                &= \begin{bmatrix} u_1+(v_1+w_1), & u_2+(v_2+w_2), & \cdots & u_n+(v_n+w_n) \end{bmatrix} \\
                &= \textbf{u}+(\textbf{v}+\textbf{w})
            \end{align*}
        
        \item
            \begin{align*}
                \textbf{u}+\textbf{0}
                &= \begin{bmatrix} u_1+0, & u_2+0, & \cdots & u_n+0 \end{bmatrix} \\
                &= \begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix} = \textbf{u}
            \end{align*}
        
        \item (Excercise 1.1 24)
            \begin{align*}
                \textbf{u}+(-\textbf{u})
                &= \begin{bmatrix} u_1+(-u_1), & u_2+(-u_2), & \cdots & u_n+(-u_n) \end{bmatrix} \\
                &= \begin{bmatrix} 0, & 0, & \cdots & 0 \end{bmatrix} = \textbf{0}
            \end{align*}
        
        \item (Excercise 1.1 24)
            \begin{align*}
                c(\textbf{u}+\textbf{v})
                &= c \begin{bmatrix} u_1+v_1, & u_2+v_2, & \cdots & u_n+v_n \end{bmatrix} \\
                &= \begin{bmatrix} c(u_1+v_1), & c(u_2+v_2), & \cdots & c(u_n+v_n) \end{bmatrix} \\
                &= \begin{bmatrix} cu_1+cv_1, & cu_2+cv_2, & \cdots & cu_n+cv_n \end{bmatrix} \\
                &= \begin{bmatrix} cu_1, & cu_2, & \cdots & cu_n \end{bmatrix} +
                   \begin{bmatrix} cv_1, & cv_2, & \cdots & cv_n \end{bmatrix} \\
                &= c\textbf{u}+c\textbf{v}
            \end{align*}
        
        \item (Excercise 1.1 24)
            \begin{align*}
                (c+d)\textbf{u}
                &= \begin{bmatrix} (c+d)u_1, & (c+d)u_2, & \cdots & (c+d)u_n \end{bmatrix} \\
                &= \begin{bmatrix} cu_1+du_1, & cu_2+du_2, & \cdots & cu_n+du_n \end{bmatrix} \\
                &= \begin{bmatrix} cu_1, & cu_2, & \cdots & cu_n \end{bmatrix} +
                   \begin{bmatrix} du_1, & du_2, & \cdots & du_n \end{bmatrix} \\
                &= c\textbf{u}+d\textbf{u}
            \end{align*}
        
       \item (Excercise 1.1 24)
            \begin{align*}
                c(d\textbf{u})
                &= c \begin{bmatrix} du_1, & du_2, & \cdots & du_n \end{bmatrix} \\
                &= \begin{bmatrix} cdu_1, & cdu_2, & \cdots & cdu_n \end{bmatrix} \\
                &= cd\begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix} \\
                &= (cd)\textbf{u}
            \end{align*}
    \end{enumerate}
\end{proof}

\section{Dot Product}

\textit{Definition.}
Let $\textbf{u}$,$\textbf{v} \in$ \Rn be $\textbf{u} = \begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix},
\textbf{v} = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}$.
Then the \textbf{dot product} $\textbf{u}\cdot\textbf{v}$ of $\textbf{u}$ and $\textbf{v}$ is defined as
\begin{displaymath}
    \textbf{u}\cdot\textbf{v}=u_1v_1+u_2v_2+\cdots+u_nv_n
\end{displaymath}

\begin{theorem}[Properties of Dot Product]
    Let \textbf{u},\textbf{v}, and \textbf{w} be vectors in \Rn and let $c$ be a scalar.
    \begin{enumerate}
        \item
        $\textbf{u}\cdot\textbf{v}=\textbf{v}\cdot\textbf{u}$ (Commutative Property of Dot Product Operation)
        \item
        $\textbf{u}\cdot(\textbf{v}+\textbf{w})=\textbf{u}\cdot\textbf{v}+\textbf{u}\cdot\textbf{w}$ (Distributive Property of Dot Product Operation over Vector Addition)
        \item
        $(c\textbf{u})\cdot\textbf{v}=c(\textbf{u}\cdot\textbf{v})$
        \item
        $\textbf{u}\cdot\textbf{u}\ge0$ and $\textbf{u}\cdot\textbf{u}=0$ if and only if $\textbf{u}=\textbf{0}$
    \end{enumerate}
\end{theorem}

\begin{proof}
    Let $\textbf{u}, \textbf{v}, \textbf{w} \in$ \Rn be
    $\textbf{u} = \begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix},
    \textbf{v} = \begin{bmatrix} v_1, & v_2, & \cdots & v_n \end{bmatrix},
    \textbf{w} = \begin{bmatrix} w_1, & w_2, & \cdots & w_n \end{bmatrix}$,
    and let $c$ be a scalar.
    
    \begin{enumerate}
        \item
            \begin{align*}
                \textbf{u}\cdot\textbf{v}
                &= u_1v_1+u_2v_2+\cdots+u_nv_n \\
                &= v_1u_1+v_2u_2+\cdots+v_nu_n = \textbf{v}\cdot\textbf{u}
            \end{align*}
        \item
            \begin{align*}
                \textbf{u}\cdot(\textbf{v}+\textbf{w})
                &= \begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix} \cdot
                   \begin{bmatrix} v_1+w_1, & v_2+w_2, & \cdots & v_n+w_n \end{bmatrix} \\
                &= u_1(v_1+w_1) + u_2(v_2+w_2) + \cdots + u_n(v_n+w_n) \\
                &= (u_1v_1+u_2v_2+\cdots+u_nv_n)+(u_1w_1+u_2w_2+\cdots+u_nw_n) \\
                &= \textbf{u}\cdot\textbf{v}+\textbf{u}\cdot\textbf{w}
            \end{align*}
        \item
            \begin{align*}
                (c\textbf{u})\cdot\textbf{v}
                &= \begin{bmatrix} cu_1, & cu_2, & \cdots & cu_n \end{bmatrix} \cdot
                   \begin{bmatrix} v_1, & v_2, & \cdots & v_n \end{bmatrix} \\
                &= cu_1v_1 + cu_2v_2 + \cdots + cu_nv_n \\
                &= c(u_1v_1 + u_2v_2 + \cdots + u_nv_n) = c(\textbf{u}\cdot\textbf{v})
            \end{align*}
        \item
            \begin{displaymath}
                \textbf{u}\cdot\textbf{u} = u_1^2 + u_2^2 + \cdots + u_n^2 \ge 0
            \end{displaymath}
            and since $u_1^2+u_2^2+\cdots+u_n^2=0$ if and only if $u_1=u_2=\cdots=u_n=0$, $\textbf{u}\cdot\textbf{u}=0$ if and only if $\textbf{u}=\textbf{0}$.
    \end{enumerate}
\end{proof}

\noindent
\textit{Definition.}
The \textbf{length} or \textbf{norm} of $\textbf{v} \in$ \Rn is defined as
\begin{displaymath}
    \Vert\textbf{v}\Vert=\sqrt{\textbf{v}\cdot\textbf{v}}
\end{displaymath}

\begin{theorem}[Properties of Norm]
    For all vectors $\textbf{u}$, $\textbf{v}$ in \Rn and scalar $c$,
    \begin{enumerate}
        \item $\Vert\textbf{v}\Vert=0$ if and only if $\textbf{v}=\textbf{0}$
        \item $\Vert c\textbf{v}\Vert=|c|\Vert\textbf{v}\Vert$
    \end{enumerate}
\end{theorem}

\begin{proof}
    Let $\textbf{u}, \textbf{v} \in$ \Rn be
    $\textbf{u} = \begin{bmatrix} u_1, & u_2, & \cdots & u_n \end{bmatrix},
    \textbf{v} = \begin{bmatrix} v_1, & v_2, & \cdots & v_n \end{bmatrix}$,
    and let $c$ be a scalar.
    \begin{enumerate}
        \item
        Since $\Vert\textbf{v}\Vert=\sqrt{v_1^2+v_2^2+\cdots+v_n^2}=0$ if and only if $v_1=v_2=\cdots=v_n=0$, $\Vert\textbf{v}\Vert=0$ if and only if $\textbf{v}=\textbf{0}$.
        \item
        \begin{align*}
            \Vert c\textbf{v}\Vert
            &= \sqrt{(cv_1)^2 + (cv_2)^2 + \cdots + (cv_n)^2} \\
            &= |c| \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} = |c|\Vert\textbf{v}\Vert
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{theorem}[The Cauchy-Schwarz Inequality]
    For all vectors $\textbf{u}$, $\textbf{v}$ in \Rn,
    $|\textbf{u}\cdot\textbf{v}|\le\Vert\textbf{u}\Vert\Vert\textbf{v}\Vert$.
\end{theorem}

\begin{proof}
    (Exercise 1.2 71) For any $t \in \mathbb{R}$, $(u_1t-v_1)^2+(u_2t-v_2)^2+\cdots+(u_nt-v_n)^2 \ge 0$.
    Therefore, the determinant of the quatratic equation $(u_1^2 + u_2^2 + \cdots + u_n^2)t^2 - 2(u_1v_1 + u_2v_2 + \cdots + u_nv_n)t + (v_1^2 + v_2^2 + \cdots + v_n^2) = 0$ is negative or $0$. Then
    \begin{align*}
        D/4=(u_1v_1 + u_2v_2 + \cdots + u_nv_n)^2 &- (u_1^2 + u_2^2 + \cdots + u_n^2)(v_1^2 + v_2^2 + \cdots + v_n^2) \le 0 \\
        |u_1v_1 + u_2v_2 + \cdots + u_nv_n| &\le \sqrt{u_1^2 + u_2^2 + \cdots + u_n^2}\sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} \\
        \therefore |\textbf{u}\cdot\textbf{v}| &\le \Vert\textbf{u}\Vert\Vert\textbf{v}\Vert
    \end{align*}
\end{proof}

\begin{theorem}[The Triangle Inequality]
    For all vectors $\textbf{u}$, $\textbf{v}$ in \Rn,
    $\Vert\textbf{u}+\textbf{v}\Vert \le \Vert\textbf{u}\Vert + \Vert\textbf{v}\Vert$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \Vert\textbf{u}+\textbf{v}\Vert^2
        &= (\textbf{u}+\textbf{v})\cdot(\textbf{u}+\textbf{v}) \\
        &= \textbf{u}\cdot\textbf{u}+2(\textbf{u}\cdot\textbf{v})+\textbf{v}\cdot\textbf{v} \\
        &\le \Vert\textbf{u}\Vert^2 + 2|\textbf{u}\cdot\textbf{v}| + \Vert\textbf{v}\Vert^2 \\
        &\le \Vert\textbf{u}\Vert^2 + 2\Vert\textbf{u}\Vert\Vert\textbf{v}\Vert + \Vert\textbf{v}\Vert^2 \\
        &= (\Vert\textbf{u}\Vert + \Vert\textbf{v}\Vert)^2
    \end{align*}
\end{proof}

\noindent
\textit{Definition.}
The \textbf{distance} between $\textbf{u}, \textbf{v} \in$ \Rn is defined as
\begin{displaymath}
    \textnormal{d}(\textbf{u},\textbf{v})=\Vert\textbf{u}-\textbf{v}\Vert
\end{displaymath}
\textit{Definition.}
The \textbf{angle} between nonzero vectors $\textbf{u}, \textbf{v} \in$ \Rn is defined as
\begin{displaymath}
    \cos\theta=\frac{\textbf{u}\cdot\textbf{v}}{\Vert\textbf{u}\Vert\Vert\textbf{v}\Vert}
\end{displaymath}
* \textit{Note.} Since $|\textbf{u}\cdot\textbf{v}| \le \Vert\textbf{u}\Vert\Vert\textbf{v}\Vert$, $-1\le\cos\theta\le1$, which corresponds with properties of cosine function.
\\
\textit{Definition.} $\textbf{u}$ and $\textbf{v}$ are \textbf{orthogonal} to each other if and only if $\textbf{u}\cdot\textbf{v}=0$.
Since $\textbf{0}\cdot\textbf{v}=0$ for every vector $\textbf{v}$ in \Rn, $\textbf{0}$ is orthonogal with every vector.

\begin{theorem}[Pythagoras' Theorem]
    For vectors $\textbf{u}$ and $\textbf{v}$ in \Rn, $\Vert\textbf{u}+\textbf{v}\Vert^2 = \Vert\textbf{u}\Vert^2 + \Vert\textbf{v}\Vert^2$ if and only if $\textbf{u}$ and $\textbf{v}$ are orthogonal.
\end{theorem}

\begin{proof}
    Since $\Vert\textbf{u}+\textbf{v}\Vert^2 = \Vert\textbf{u}\Vert^2 + 2(\textbf{u}\cdot\textbf{v}) + \Vert\textbf{v}\Vert^2$, $\Vert\textbf{u}+\textbf{v}\Vert^2 = \Vert\textbf{u}\Vert^2 + \Vert\textbf{v}\Vert^2$ if and only if $\textbf{u}\cdot\textbf{v}=0$.
\end{proof}

\noindent
\textit{Definition.} For vectors $\textbf{u}, \textbf{v} \in$ \Rn and $\textbf{u} \neq \textbf{0}$, the \textbf{projection of $\textbf{v}$ onto $\textbf{u}$} is denoted as $\textnormal{proj}_\textbf{u}(\textbf{v})$ and defined as
\begin{displaymath}
    \textnormal{proj}_\textbf{u}(\textbf{v})=\left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\right)\textbf{u}
\end{displaymath}
\\
\textit{Derivation.} The projection of $\textbf{v}$ onto $\textbf{u}$ is scalar multiple of $\textbf{u}$, and its scale is $\Vert\textbf{v}\Vert\cos\theta$. Therefore,
\begin{align*}
    \textnormal{proj}_\textbf{u}(\textbf{v})
    &= \Vert\textbf{v}\Vert\cos\theta\left(\frac{1}{\Vert\textbf{u}\Vert}\right)\textbf{u} \\
    &= \Vert\textbf{v}\Vert\left(\frac{\textbf{u}\cdot\textbf{v}}{\Vert\textbf{u}\Vert\Vert\textbf{v}\Vert}\right)\left(\frac{1}{\Vert\textbf{u}\Vert}\right)\textbf{u} \\
    &= \left(\frac{\textbf{u}\cdot\textbf{v}}{\Vert\textbf{u}\Vert^2}\right)\textbf{u} \\
    &= \left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\right)\textbf{u}
\end{align*}

\section{Geometry in Vectors}

\begin{table} [h]
	\begin{center}
		\begin{tabular}{cccccc}
			\hline
			& \textbf{Normal Form} & \textbf{General Form} & \textbf{Vector Form} & \textbf{Parametric Form} \\
			\hline
			\textbf{Lines} &
			$\textbf{n}\cdot\textbf{x} = \textbf{n}\cdot\textbf{p}$ &
			$ax +  by = c$ &
			$\textbf{x} = \textbf{p} + t\textbf{d}$ &
			$\begin{cases}
				x = p_x + td_x \\
				y = p_y + td_y
			\end{cases}$ \\
			\hline
		\end{tabular}
		\\ ($t \in \mathbb{R}$)
		\caption{
			Equations of Lines in $\mathbb{R}^2$.
		}
	\end{center}
\end{table}

\begin{table}[h]
	\begin{center}
		\begin{tabular}{cccccc}
			\hline
			& \textbf{Normal Form} & \textbf{General Form} & \textbf{Vector Form} & \textbf{Parametric Form} \\
			\hline
			\textbf{Lines} &
			$\begin{cases}
				 \textbf{n}_1\cdot\textbf{x} = \textbf{n}_1\cdot\textbf{p}_1 \\
				 \textbf{n}_2\cdot\textbf{x} = \textbf{n}_2\cdot\textbf{p}_2
			\end{cases}$ &
			$\begin{cases}
				a_1x + b_1y + c_1z = d_1 \\
				a_2x + b_2y + c_2z = d_2
			\end{cases}$ &
			$\textbf{x} = \textbf{p} + t\textbf{d}$ & 
			$\begin{cases}
				x = p_x + td_x \\
				y = p_y + td_y \\
				z = p_z + td_z
			\end{cases}$ \\
			\hline
			\textbf{Planes} &
			$\textbf{n}\cdot\textbf{x} = \textbf{n}\cdot\textbf{p}$ &
			$ax + by + cz = d$ &
			$\textbf{x} = \textbf{p} + s\textbf{u} + t\textbf{v}$ &
			$\begin{cases}
				x = p_x + su_x + tv_x \\
				y = p_y + su_y + tv_y \\
				z = p_z + su_z + tv_z
			\end{cases}$ \\
			\hline
		\end{tabular}
		\\ ($t, s \in \mathbb{R}$)
		\caption{
			Equations of Lines and Planes in $\mathbb{R}^3$.
		}
	\end{center}
\end{table}

\noindent
The distance from the point $A$ to a line $l$: $\textbf{x}=\textbf{p}+t\textbf{d}$ is represented as
\begin{align*}
	\textnormal{d}(A,l)
	&=\left\| (\textbf{a}-\textbf{p})-\textnormal{proj}_\textbf{d}(\textbf{a}-\textbf{p})\right\| \\
	&= \left\| (\textbf{a}-\textbf{p}) - \left(\frac{\textbf{d}\cdot(\textbf{a}-\textbf{p})}{\textbf{d}\cdot\textbf{d}}\right)\textbf{d} \right\|
\end{align*}
In $\mathbb{R}^2$, if $A(x_0,y_0)$ and $l$ : $ax + by + c = 0$,
\begin{align*}
	\textnormal{d}(A, l) = \frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}
\end{align*}
\textit{Derivation.} (Excercise 1.3 39)
Let $\textbf{a}=\begin{bmatrix} x_0 \\ y_0 \end{bmatrix}$,
$\textbf{p}=\begin{bmatrix} x_1 \\ y_1 \end{bmatrix}$,
and $\textbf{d}=\begin{bmatrix} b \\ -a \end{bmatrix}$.
\begin{align*}
	\textnormal{d}(A,l)
	&= \left\|(\textbf{a}-\textbf{p})-\textnormal{proj}_\textbf{d}(\textbf{a}-\textbf{p})\right\| \\
	&= \left\|\begin{bmatrix}
		(x_0-x_1)-\frac{b(x_0-x_1)-a(y_0-y_1)}{a^2+b^2}b \\
		(y_0-y_1)+\frac{b(x_0-x_1)-a(y_0-y_1)}{a^2+b^2}a
	\end{bmatrix}\right\| \\
	&= \left\|\begin{bmatrix}
		\frac{a}{a^2+b^2}\{(ax_0+by_0) - (ax_1+by_1)\} \\
		\frac{b}{a^2+b^2}\{(ax_0+by_0) - (ax_1+by_1)\}
	\end{bmatrix}\right\| \\
	&= \left\|\begin{bmatrix}
	\frac{a}{a^2+b^2}(ax_0+by_0+c) \\
	\frac{b}{a^2+b^2}(ax_0+by_0+c)
	\end{bmatrix}\right\| \\
	&= \frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}
\end{align*}

\noindent
The distance from the point $A$ to a plane $\mathcal{P}$ (or a hyperplane) with a normal vector $\textbf{n}$, and point $P$ is on the plane is represented as
\begin{align*}
	\textnormal{d}(A,\mathcal{P})
	&= 
	\left\| \textnormal{proj}_\textbf{n}(\textbf{a}-\textbf{p}) \right\| \\
	&= \left\| \frac{\textbf{n}\cdot(\textbf{a}-\textbf{p})}{\textbf{n}\cdot\textbf{n}}\right\|  \left\| \textbf{n} \right\|
\end{align*}
In $\mathbb{R}^3$, if $A(x_0,y_0,z_0)$ and $\mathcal{P}$: $ax+by+cz+d=0$,
\begin{align*}
	\textnormal{d}(A,\mathcal{P})=\frac{|ax_0+by_0+cz_0+d|}{\sqrt{a^2+b^2+c^2}}
\end{align*}
\textit{Derivation.} (Excercise 1.3 40)
Let $\textbf{a}=\begin{bmatrix} x_0 \\ y_0 \\ z_0 \end{bmatrix}$,
$\textbf{p}=\begin{bmatrix} x_1 \\ y_1 \\ z_1 \end{bmatrix}$,
and $\textbf{n}=\begin{bmatrix} a \\ b \\ c \end{bmatrix}$.
\begin{align*}
	\textnormal{d}(A,\mathcal{P})
	&= |\frac{\textbf{n}\cdot(\textbf{a}-\textbf{p})}{\textbf{n}\cdot\textbf{n}}| \Vert \textbf{n} \Vert \\
	&= |\frac{a(x_0-x_1)+b(y_0-y_1)+c(z_0-z_1)}{a^2+b^2+c^2}|\sqrt{a^2+b^2+c^2} \\
	&= \frac{|(ax_0+by_0+cz_0)-(ax_1+by_1+cz_1)|}{\sqrt{a^2+b^2+c^2}} \\
	&= \frac{|ax_0+by_0+cz_0+d|}{\sqrt{a^2+b^2+c^2}}
\end{align*}

\section{Solutions for Excercises Worthy to Solve}
\begin{enumerate}
	\item \textbf{Excercise 1.2 55}
		\begin{proof}
			\begin{align*}
				\textnormal{d}(\textbf{u},\textbf{v})
				&= \Vert\textbf{u}-\textbf{v}\Vert \\
				&= \Vert (-1)(\textbf{v}-\textbf{u})\Vert
				= |-1|\Vert\textbf{v}-\textbf{u}\Vert \\
				&= \Vert\textbf{v}-\textbf{u}\Vert = \textnormal{d}(\textbf{v},\textbf{u})
			\end{align*}
		\end{proof}
	\item \textbf{Excercise 1.2 56}
		\begin{proof}
			\begin{align*}
			\textnormal{d}(\textbf{u},\textbf{w}) 
			= \Vert\textbf{u} - \textbf{w}\Vert
			&= \Vert(\textbf{u}-\textbf{v})+(\textbf{v}-\textbf{w})\Vert \\
			&\le \Vert\textbf{u}-\textbf{v}\Vert + \Vert\textbf{v}-\textbf{w}\Vert
			= \textnormal{d}(\textbf{u},\textbf{v}) + \textnormal{d}(\textbf{v},\textbf{w})
			\end{align*}
		\end{proof}
	\item \textbf{Excercise 1.2 57}
		\begin{proof}
			By Theorem 1.3 (c), $\textnormal{d}(\textbf{u},\textbf{v}) = \Vert\textbf{u}-\textbf{v}\Vert = 0$ if and only if $\textbf{u}-\textbf{v}=\textbf{0}$. Thus, $\textnormal{d}(\textbf{u},\textbf{v})=0$ if and only if $\textbf{u}=\textbf{v}$.
		\end{proof}
	\item \textbf{Excercise 1.2 59}
		\begin{proof}
			\begin{align*}
				\Vert\textbf{u}\Vert
				&= \Vert(\textbf{u}-\textbf{v})+\textbf{v}\Vert \\
				&\le \Vert\textbf{u}-\textbf{v}\Vert + \Vert\textbf{v}\Vert \\
				\therefore \Vert\textbf{u}&-\textbf{v}\Vert \ge \Vert\textbf{u}\Vert - \Vert\textbf{v}\Vert
			\end{align*}
		\end{proof}
	\item \textbf{Excercise 1.2 60} \\\\
		\textit{Solution.}
		A well-known counterexample for $\textbf{u}\cdot\textbf{v}=\textbf{u}\cdot\textbf{w} \rightarrow \textbf{v}=\textbf{w}$ is the case when $\textbf{u}\cdot\textbf{v}=\textbf{u}\cdot\textbf{w}=0$. Vectors orthogonal with $\textbf{u}$ satisfy the condition, though they are not equal.
	\item \textbf{Excercise 1.2 69}
		\begin{proof}
			\begin{align*}
				\textbf{u}\cdot\textnormal{proj}_\textbf{u}(\textbf{v})
				&= \textbf{u}\cdot\left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\textbf{u}\right) \\
				&= \left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\right)\left(\textbf{u}\cdot\textbf{u}\right) = \textbf{u}\cdot\textbf{v} \\
				\therefore \textbf{u}\cdot(\textbf{v}-\textnormal{proj}_\textbf{u}(\textbf{v})) = \textbf{u}\cdot\textbf{v} &- \textbf{u}\cdot\textnormal{proj}_\textbf{u}(\textbf{v}) = \textbf{u}\cdot\textbf{v} - \textbf{u}\cdot\textbf{v} = 0
			\end{align*}
			Therefore, $\textbf{u}$ is orthogonal to $\textbf{v}-\textnormal{proj}_\textbf{u}(\textbf{v})$.
		\end{proof}
	\item \textbf{Excercise 1.2 70}
		\begin{proof}
			\begin{enumerate}
			\item
				\begin{align*}
					\textnormal{proj}_\textbf{u}(\textnormal{proj}_\textbf{u}(\textbf{v}))
					&= \left(\frac{\textbf{u}\cdot\left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\textbf{u}\right)}{\textbf{u}\cdot\textbf{u}}\right)\textbf{u} \\
					&= \left(\frac{\left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\right)\left(\textbf{u}\cdot\textbf{u}\right)}{\textbf{u}\cdot\textbf{u}}\right)\textbf{u} \\
					&= \left(\frac{\textbf{u}\cdot\textbf{v}}{\textbf{u}\cdot\textbf{u}}\right)\textbf{u} = \textnormal{proj}_\textbf{u}(\textbf{v})
				\end{align*}
			\item By Excercise 1.2 69, $\textbf{u}$ is orthogonal to $\textbf{v}-\textnormal{proj}_\textbf{u}(\textbf{v})$, thus $\textbf{u}\cdot(\textbf{v}-\textnormal{proj}_\textbf{u}(\textbf{v}))=0$. Therefore, $\textnormal{proj}_\textbf{u}(\textbf{v}-\textnormal{proj}_\textbf{u}(\textbf{v})) = \textbf{0}$.
			\end{enumerate}
		\end{proof}
	\item \textbf{Excercise 1.3 17}
		\begin{proof}
			Consider two lines with slopes $m_1$ and $m_2$. Then the direction vectors of two lines are $\begin{bmatrix} 1 \\ m_1 \end{bmatrix}$, and $\begin{bmatrix} 1 \\ m_2 \end{bmatrix}$, respectively. Since two vectors are orthogonal (or, perpendicular) if and only if $\begin{bmatrix} 1 \\ m_1 \end{bmatrix} \cdot \begin{bmatrix} 1 \\ m_2 \end{bmatrix} = 1 + m_1m_2 = 0$, two lines are perpendicular if and only if $m_1m_2 = -1$.
		\end{proof}
	\item \textbf{Excercise 1.3 41}
		\begin{proof}
			Suppose $\textbf{n}_1\cdot\textbf{x}_1 = c_1$ and $\textbf{n}_2\cdot\textbf{x}_2 = c_2$. Then the distance between two lines is
			\begin{align*}
				\Vert\textnormal{proj}_\textbf{n}(\textbf{x}_2-\textbf{x}_1)\Vert
				&= |\frac{\textbf{n}\cdot(\textbf{x}_2-\textbf{x}_1)}{\textbf{n}\cdot\textbf{n}}|\Vert\textbf{n}\Vert \\
				&= \frac{|\textbf{n}\cdot\textbf{x}_2 - \textbf{n}\cdot\textbf{x}_1|}{\Vert\textbf{n}\Vert} \\
				&= \frac{|c_2 - c_1|}{\Vert\textbf{n}\Vert}
			\end{align*}
		\end{proof}
	\item \textbf{Excercise 1.3 42}
		\begin{proof}
			Suppose $\textbf{n}_1\cdot\textbf{x}_1 = d_1$ and $\textbf{n}_2\cdot\textbf{x}_2 = d_2$. Then the distance between two planes is
			\begin{align*}
			\Vert\textnormal{proj}_\textbf{n}(\textbf{x}_2-\textbf{x}_1)\Vert
			&= |\frac{\textbf{n}\cdot(\textbf{x}_2-\textbf{x}_1)}{\textbf{n}\cdot\textbf{n}}|\Vert\textbf{n}\Vert \\
			&= \frac{|\textbf{n}\cdot\textbf{x}_2 - \textbf{n}\cdot\textbf{x}_1|}{\Vert\textbf{n}\Vert} \\
			&= \frac{|d_2 - d_1|}{\Vert\textbf{n}\Vert}
			\end{align*}
		\end{proof}
	\item \textbf{Excercise 1.3 47} \\
		\textit{Solution.}
			\begin{align*}
				\textbf{p}\cdot\textbf{n}
				&= (\textbf{v}-c\textbf{n})\cdot\textbf{n} \\
				&= \textbf{v}\cdot\textbf{n} - c\Vert\textbf{n}\Vert^2 = 0 \\
				\therefore c &= \frac{\textbf{v}\cdot\textbf{n}}{\Vert\textbf{n}\Vert^2} \\
				\textbf{p} &= \textbf{v} - \frac{\textbf{v}\cdot\textbf{n}}{\Vert\textbf{n}\Vert^2}\textbf{n}
			\end{align*}
\end{enumerate}