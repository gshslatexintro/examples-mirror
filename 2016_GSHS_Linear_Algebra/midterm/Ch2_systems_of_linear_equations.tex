\chapter{Systems of Linear Equations}

\section{Terminology}
\textit{Definition.} A vector $\textbf{v}$ is a \textbf{linear combination} of vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ if there exist scalars $c_1, c_2, \cdots, c_n$ such that
\begin{displaymath}
	c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = \textbf{0}
\end{displaymath}
The scalars $c_1, c_2, \cdots, c_n$ are called \textbf{coefficients} of linear combination.
\\\\
\textit{Definition.} A \textbf{linear equation} in the $n$ variables $x_1, x_2, \cdots, x_n$ is an equation that can be written in the form of
\begin{displaymath}
	a_1x_1 + a_2x_2 + \cdots + a_nx_n = b
\end{displaymath}
where the \textbf{coefficients} $a_1, a_2, \cdots, a_n$ and the \textbf{constant term} $b$ are constants.
\\\\
A \textbf{set of linear equations} is a finite set of linear equations with same variables. A \textbf{solution} of a linear equation $a_1x_1 + a_2x_2 + \cdots + a_nx_n = b$ is a vector $\begin{bmatrix}
	x_1 & x_2 & \cdots & x_n
\end{bmatrix}$ which satisfies the equation. A solution of a set of linear equations is a vector which is simultaneously a solution of all linear equations in the system. A \textbf{solution set} of a system of linear equations is the set of all solutions of the system.
\\\\
A system of linear equations is \textbf{consistent} if there exists a solution. \textbf{Inconsistent} set of linear equations has an empty solution set.
\noindent Two linear systems are \textbf{equivalent} if they have same solution set.
\\\\
\noindent The \textbf{coefficient matrix} contains the coefficients of variables in the set of linear equations. The \textbf{augmented matrix} is the coefficient matrix augmented by a vector containing constant terms. For the system
\begin{align*}
	\begin{cases}
	a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
	a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
	\cdots \\
	a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
	\end{cases}
\end{align*} the coefficient matrix is
\begin{align*}
A = \begin{bmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots &        & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\end{align*} and the augmented matrix is
\begin{align*}
	\begin{bmatrix}
		A | \textbf{b}
	\end{bmatrix} = \begin{bmatrix}[cccc|c]
		a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
		a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
		\vdots & \vdots &        & \vdots & \vdots \\
		a_{m1} & a_{m2} & \cdots & a_{mn} & b_m \\
	\end{bmatrix}
\end{align*}
\textit{Definition.} A matrix is in \textbf{row echelon form} (REF) if:
\begin{enumerate}
	\item All rows consisting entirely of zeros are at the bottom.
	\item The \textbf{leading entry} (the first nonzero entry) of each rows is located to the left of any leading entries below it.
\end{enumerate}
\textit{Definition.} A matrix is in \textbf{reduced row echelon form} (RREF) if:
\begin{enumerate}
	\item It is in REF.
	\item All leading entries are 1. (\textbf{leading 1})
	\item Each columns containing a leading 1 has 0 everywhere else.
\end{enumerate}

\noindent REF of a matrix is not unique, but all matrices have unique RREF.

\noindent \\ \textit{Definition.} A system of linear equations is \textbf{homogeneous} if the constant term in each equation is zero.

\section{Solving Linear Systems}

A system of linear equations with \textit{real coefficients} has either a unique solution, or infinitely many solutions, or no solutions.
\begin{proof} (Another proof is shown at Theorem 3.22)
	Consider a consistent set of $m$ linear equations
	\begin{align*}
		\begin{cases}
			a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
			a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
			\cdots \\
			a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
		\end{cases}
	\end{align*}
	Suppose this set has more than one solutions. Then there exists two solutions $\textbf{x} = \begin{bmatrix}
	x_1 & x_2 & \cdots & x_n
	\end{bmatrix}, \textbf{y} = \begin{bmatrix}
	y_1 &y _2 & \cdots & y_n
	\end{bmatrix}$, where $\textbf{x} \neq \textbf{y}$. For any real number $k$,
	\begin{align*}
		&a_{i1}(kx_1 + (1-k)y_1) + a_{i2}(kx_2 + (1-k)y_2) + \cdots + a_{in}(kx_n + (1-k)y_n) \\
		&= k(a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n) + (1-k)(a_{i1}y_1 + a_{i2}y_2 + \cdots + a_{in}y_n) \\
		&= kb_i + (1-k)b_i = b_i
	\end{align*}
	where $1 \le i \le m$.
	Thus, a vector
	\begin{align*}
		\textbf{v}_k = \begin{bmatrix}
		kx_1 + (1-k)y_1 & kx_2 + (1-k)y_2 & \cdots & kx_n + (1-k)y_n
		\end{bmatrix}
	\end{align*} is a also solution of the set of linear equations.
	Therefore, if the set of linear equations has more than one solutions, then there are infinitely many solutions.
\end{proof}

\noindent \textit{Definition.} The \textbf{elementary row operations}
\begin{enumerate}
	\item Interchanging two rows ($R_i \leftrightarrow R_j$)
	\item Multiplying nonzero constant to a row ($kR_i$)
	\item Adding a multiple of a row to another row ($R_i + kR_j$)
\end{enumerate}
can be performed to a matrix, and the matrices before and after applying the elementary row operations are equivalent.
Applying certain sequence of elementary row operations to bring a matrix into REF is called \textbf{row reduction}.

\noindent \\ \textit{Definition.} Matrices $A$ and $B$ are \textbf{row equivalent} if there exists a sequence of elementary row operation that converts $A$ to $B$.

\begin{theorem}
	Matrices $A$ and $B$ are row equivalent if and only if they can be reduced to same REF.
\end{theorem}
\begin{proof}
	Suppose that matrices $A$ and $B$ can be reduced to REF $R$. Then there exists a sequence of elementary row operations which converts $B$ to $R$. Reversing the sequence will convert $R$ to $B$. Then combining the sequence of operations $A \rightarrow R$ and $R \rightarrow B$ will convert $A$ to $B$. Therefore, $A$ and $B$ are row equivalent.
\end{proof}

\noindent \textit{Definition.} The \textbf{rank} of a matrix is the number of nonzero rows in REF of a matrix. \\

\noindent The variables in the linear system corresponding to leading entires in REF of coefficient matrix is called \textbf{leading variables}. The rest are called \textbf{free variables}. \\

\noindent \textit{Note.} A system of linear equations with at least one free variable has infinitely many solutions.

\begin{theorem}[The Rank Theorem]
	Let $A$ be the coefficient matrix of a system of linear equations with $n$ variables. If the system is consistent, then \begin{align*}
		\textnormal{number of free variables} = n - \textnormal{rank}(A)
	\end{align*}
\end{theorem}

\begin{proof}
	The number of leading variables in the system is equal to the number of nonzero rows of the coefficient matrix $A$, which is rank$(A)$. Therefore, the number of free variables is $n-$rank$(A)$.
\end{proof}

\begin{theorem}
	A homogeneous system of linear equations $[A|\textbf{0}]$ with $n$ variables and $m$ equations has infinitely many solutions if $m<n$.
\end{theorem}

\begin{proof}
	Since there exists a trivial solution $\textbf{0}$, the system is consistent. By the Rank Theorem (Thm 2.2),
	\begin{align*}
		\textnormal{number of free variables} = n - \textnormal{rank}(A) \ge n - m > 0
	\end{align*} Therefore there exists at least one free variable, hence the system has infinitely many solutions.
\end{proof}

\section{Solving Linear Systems : Example}

\noindent The \textbf{Gaussian Elimination} is an algorithm which solves a linear system. The procedure is
\begin{enumerate}
	\item Reduce the augmented matrix of the system into REF.
	\item Using back substitution, solve the REF of the matrix.
\end{enumerate}

\noindent The \textbf{Gauss-Jordan Elimination} also solves a linear system, but in easier way. The procedure is
\begin{enumerate}
	\item Reduce the augmented matrix of the system into RREF.
	\item Solve the leading variables in terms of free variables.
\end{enumerate}

\noindent Solving a given system of linear equations \begin{align*}
	\begin{cases}
	   	 x_1 -  x_2 - x_3 + 2x_4 = 1 \\
		2x_1 - 2x_2 - x_3 + 3x_4 = 3 \\
		-x_1 +  x_2 - x_3        = -3
	\end{cases}
\end{align*} using Gaussian Elimination starts from representing the system into augmented matrix \begin{align*}
	\begin{bmatrix}[cccc|c]
		 1 & -1 & -1 & 2 &  1 \\
		 2 & -2 & -1 & 3 &  3 \\
		-1 &  1 & -1 & 0 & -3
	\end{bmatrix}
\end{align*}
Then reduce the augmented matrix using elementary row operations.
\begin{align*}
	\begin{bmatrix}[cccc|c]
		1 & -1 & -1 & 2 &  1 \\
		2 & -2 & -1 & 3 &  3 \\
		-1 &  1 & -1 & 0 & -3
	\end{bmatrix} \xrightarrow{R_2 - 2R_1, R_3 + R_1}
	&\begin{bmatrix}[cccc|c]
		1 & -1 & -1 &  2 &  1 \\
		0 &  0 &  1 & -1 &  1 \\
		0 &  0 & -2 &  2 & -2
	\end{bmatrix} \\
	\xrightarrow{R_3 + 2R_2}
	&\begin{bmatrix}[cccc|c]
		1 & -1 & -1 &  2 &  1 \\
		0 &  0 &  1 & -1 &  1 \\
		0 &  0 &  0 &  0 &  0
	\end{bmatrix}
\end{align*}
The system is consistent because constants of zero-rows are all 0. (If not, the system will be inconsistent) The leading entries are in first and third column, so leading variables are $x_1$ and $x_3$, and free variables are $x_2$ and $x_4$. Using back substitution, solve $x_1$ and $x_3$ in terms of $x_2$ and $x_4$.
\begin{align*}
	x_3 - x_4 = 1 &\rightarrow x_3 = x_4 + 1 \\
	x_1 - x_2 - x_3 + 2x_4 = 1 &\rightarrow x_1 = x_2 - x_4 + 2
\end{align*}
You can obtain same result with reducing REF into RREF.
\begin{align*}
	\begin{bmatrix}[cccc|c]
	1 & -1 & -1 &  2 &  1 \\
	0 &  0 &  1 & -1 &  1 \\
	0 &  0 &  0 &  0 &  0
	\end{bmatrix} \xrightarrow{R_1 + R_2}
	\begin{bmatrix}[cccc|c]
	1 & -1 & 0 & 1 & 2 \\
	0 & 0 & 1 & -1 & 1 \\
	0 & 0 & 0 & 0 & 0
	\end{bmatrix}
\end{align*}
Representing the variables into form of vector will give you the solution set. ($x_2$ corresponds to $s$ and $x_4$ to $t$ in the solution.)
\begin{align*}
	\begin{bmatrix}
		x_1 \\ x_2 \\ x_3 \\ x_4
	\end{bmatrix} =
	\begin{bmatrix}
		2 \\ 0 \\ 1 \\ 0
	\end{bmatrix} + s
	\begin{bmatrix}
		1 \\ 1 \\ 0 \\ 0
	\end{bmatrix} + t
	\begin{bmatrix}
		-1 \\ 0 \\ 1 \\ 1
	\end{bmatrix} (s,t \in \mathbb{R})
\end{align*}

\section{Spanning Sets and Linear Independence}

\begin{theorem}
	A system of linear equations with augmented matrix $[A|\textbf{b}]$ is consistent if and only if $\textbf{b}$ is a linear combination of the columns of $A$.
\end{theorem}

\begin{proof}
	Suppose a system of linear equations with augmented matrix
	\begin{align*}
		[A|\textbf{b}]=
		\begin{bmatrix} [cccc|c]
			a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
			a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
			\vdots & \vdots & & \vdots & \vdots \\
			a_{m1} & a_{m2} & \cdots & a_{mn} & b_m
		\end{bmatrix}
	\end{align*} is consistent. Then there exist real numbers $c_1, c_2, \cdots, c_n$ which satisfy
	\begin{align*}
		a_{i1}c_1 + a_{i2}c_2 + \cdots + a_{in}c_n = b_i, \textnormal{for all } 1 \le i \le m
	\end{align*}
	Therefore, $b$ can be represented with linear combination of columns of $A$.
	\begin{align*}
		\textbf{b} = \begin{bmatrix}
			b_1 \\ b_2 \\ \vdots \\ b_m
		\end{bmatrix} = c_1 \begin{bmatrix}
			a_{11} \\ a_{21} \\ \vdots \\ a_{m1}
		\end{bmatrix} + c_2 \begin{bmatrix}
			a_{12} \\ a_{22} \\ \vdots \\ a_{m2}
		\end{bmatrix} + \cdots + c_n \begin{bmatrix}
			a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}
		\end{bmatrix}
	\end{align*}
	Similarly, if $b$ is a linear combination of the columns of $A$, the coefficients of linear combination satisfy the linear equations of system. Therefore, the system is consistent.
\end{proof}

\noindent \textit{Definition.} Let $S = \{\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n\}$ be a set of vectors in \Rn. Then the set of all linear combinations of $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ is called the \textbf{span} of $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ and denoted by span($\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$) or span($S$). If span($S$) = \Rn, then $S$ is a \textbf{spanning set} for \Rn.

\noindent \\ \textit{Note.} To prove that the spanning set span($S$) is equal to another set $T$, proving \textbf{both} $S \subset T$ and $T \subset S$ is essencial.

\noindent \\ \textit{Definition.} A set of vectors $\textbf{v}_1, \textbf{v}_2, \cdots \textbf{v}_n$ is \textbf{linearly dependent} if there exists scalars $c_1, c_2, \cdots c_n$, at least one of which is nonzero, such that
\begin{align*}
	c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = \textbf{0}
\end{align*}
A set of vectors which are not linearly dependent is \textbf{linearly independent}.

\begin{theorem}
	Vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent if and only if at least one of the vectors can be represented with a linear combination of other vectors.
\end{theorem}

\begin{proof}
	If vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent, there exist scalars $c_1, c_2, \cdots, c_n$ such that $c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = \textbf{0}$ and at least one of scalars is nonzero. Suppose $c_1 \neq 0$. Then
	\begin{align*}
		\textbf{v}_1 = -\frac{c_2}{c_1}\textbf{v}_2 - \cdots - \frac{c_n}{c_1}\textbf{v}_n
	\end{align*}
	Thus $\textbf{v}_1$ is represented with a linear combination of other vectors.
	\\
	Suppose that $\textbf{v}_1$ can be expressed as linear combination of $\textbf{v}_2, \textbf{v}_3, \cdots, \textbf{v}_n$. Then there exist scalars $c_2, c_3, \cdots, c_n$ which satisfy $\textbf{v}_1 = c_2\textbf{v}_2 + c_3\textbf{v}_3 + \cdots + c_n\textbf{v}_n$. Then 
	\begin{align*}
		\textbf{v}_1 - c_2\textbf{v}_2 - \cdots - c_n\textbf{v}_n = \textbf{0}
	\end{align*} thus vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent.
\end{proof}

\begin{theorem}
	Let $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ be vectors in \Rn and let $A$ be the $n \times m$ matrix $\begin{bmatrix}
		\textbf{v}_1 & \textbf{v}_2 & \cdots & \textbf{v}_n
	\end{bmatrix}$. Then $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent if and only if the homogeneous linear system with augmented matrix $[A|\textbf{0}]$ has a nontrivial solution (or, infinitely many solutions.)
\end{theorem}

\begin{proof}
	Vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent if and only if there exist scalars $c_1, c_2, \cdots, c_n$ such that $c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = \textbf{0}$ where at least one of scalars is nonzero. Since the nonzero vector $\begin{bmatrix}
		c_1 \\ c_2 \\ \vdots \\ c_n
	\end{bmatrix}$ is a solution of the linear system with augmented matrix $[A|\textbf{0}]$, the system has nontrivial solution.
	Similarly, if there exists a nontrivial solution $\begin{bmatrix}
		c_1 \\ c_2 \\ \cdots \\ c_n
	\end{bmatrix}$ of the homogeneous linear system, the components satisfy
	\begin{align*}
		c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = \textbf{0}
	\end{align*} Since at least one of $c_1, c_2, \cdots, c_n$ is nonzero, vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent.
\end{proof}

\begin{plaintheorem}[Elementary Row Operations and Linear Combination]
	(Example 2.25) If matrix $B$ is generated by applying elementary row operations at matrix $A$, then rows of $B$ can be represented as nontrivial linear combination of rows of $A$.
\end{plaintheorem}

\begin{proof}
	Let $A = \begin{bmatrix}
		\textbf{R}_1 \\ \textbf{R}_2 \\ \vdots \\ \textbf{R}_n
	\end{bmatrix}$. By Theorem 3.10, sequence of elementary row operations that would convert $A$ into $B$ will also convert $I_n$ to $E$, where $B = EA$. Since there exists the sequence of elementary row operations converting $I_n$ to $E$, $I_n$ and $E$ are row equivalent, so $\textnormal{rank}(I_n) = \textnormal{rank}(E) = n$. Then the rows of $E$ are all nonzero rows. Therefore, rows of $B = EA$ can be expressed as nontrivial linear combination of rows of $A$.
\end{proof}

\begin{theorem}
	Let $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ be row vectors in \Rn and let $A$ be the $m\times n$ matrix $\begin{bmatrix}
		\textbf{v}_1 \\ \textbf{v}_2 \\ \vdots \\ \textbf{v}_n
	\end{bmatrix}$ Then $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent if and only if rank$(A) < n$.
\end{theorem}

\begin{proof}
	Assume that $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent. By Theorem 2.5, at least one of the vectors can be written as a linear combination of other vectors. With relabeling such vector to $\textbf{v}_n$, there exist scalars $c_1, c_2, \cdots, c_{n-1}$ such that
	\begin{align*}
		\textbf{v}_n = c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_{n-1}\textbf{v}_{n-1}
	\end{align*}
	Then performing the elementary row operations $(R_n - c_1R_1), (R_n - c_2R_2), \cdots, (R_n - c_{n-1}R_{n-1})$ will create zero row in $n$th row. Therefore, rank$(A) < n$.
	
	\noindent Conversely, if rank$(A) < n$, $A$ would have zero row in its REF, thus there exists certain sequence of elementary operation that would create zero row from $A$. There exist scalars $c_1, c_2, \cdots, c_n$ such that at least one of scalars is nonzero, and
	\begin{align*}
		c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = \textbf{0}
	\end{align*} Therefore, $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ are linearly dependent.
\end{proof}

\begin{theorem}
	Any set of $m$ vectors in \Rn is linearly dependent if $m > n$.
\end{theorem}

\begin{proof}
	(also solution for Exercise 2.3 45) Let $A$ be the $m \times n$ matrix with the vectors as its rows. Since rank$(A) < n < m$, by Theorem 2.7, the vectors are linearly dependent.
\end{proof}

\section{Solutions of Exercises Worthy to Solve}
\begin{enumerate}
	\item \textbf{Exercise 2.2 39}
	\begin{proof}
		The augmented matrix of the system is $\begin{bmatrix}[cc|c]
			a & b & r \\ c & d & s
		\end{bmatrix}$. Applying elementary row operations $(cR_1), (aR_2), (R_2 - R_1)$, the REF of the augmented matrix is $\begin{bmatrix}[cc|c]
			ac & bc & rc \\ 0 & ad - bc & sa - rc
		\end{bmatrix}$. If $ad - bc \neq 0$, the rank of augmented matrix is 2, therefore the system has unique solution.
	\end{proof}
	\item \textbf{Exercise 2.2 51}
	\begin{proof}
		Let $\textbf{u} = \begin{bmatrix}
			u_1 \\ u_2 \\ u_3
		\end{bmatrix}$, $\textbf{v} = \begin{bmatrix}
			v_1 \\ v_2 \\ v_3
		\end{bmatrix}$, and $\textbf{x} = \begin{bmatrix}
			x_1 \\ x_2 \\ x_3
		\end{bmatrix}$. Since $\textbf{u}\cdot\textbf{x} = \textbf{v}\cdot\textbf{x} = 0$, $\textbf{x}$ is a solution of the linear system with augmented matrix $\begin{bmatrix} [ccc|c]
			u_1 & u_2 & u_3 & 0 \\ v_1 & v_2 & v_3 & 0
		\end{bmatrix}$. The REF of augmented matrix is $\begin{bmatrix} [ccc|c]
			u_1 & u_2 & u_3 & 0 \\ 0 & u_1v_2-u_2v_1 & u_1v_3-u_3v_1 & 0
		\end{bmatrix}$, and the solution is
		\begin{align*}
			\textbf{x} = \begin{bmatrix}
				0 \\ 0 \\ 0
			\end{bmatrix} + t\begin{bmatrix}
				u_2v_3 - u_3v_2 \\ u_3v_1 - u_1v_3 \\ u_1v_2 - u_2v_1
			\end{bmatrix}
		\end{align*} where $t \in \mathbb{R}$.
	\end{proof}
	\item \textbf{Exercise 2.2 59}
	\begin{proof}
		By Theorem 2.2, there are $n-\textnormal{rank}(A)$ free variables and $\textnormal{rank}(A)$ leading variables in the linear system. Since the values of leading variables are fixed for given values of free variables, the number of solution is $p^{n-\textnormal{rank}(A)}$.
	\end{proof}
	\item \textbf{Exercise 2.2 60}
	
	\noindent \textit{Solution.} Complications arise when we try to solve this directly in $\mathbb{Z}_6$. Therefore, we split the system to $\mathbb{Z}_2$ and $\mathbb{Z}_3$, then find the solutions simultaneously satisfying them.
	
	\noindent In $\mathbb{Z}_2$,
	\begin{align*}
		\begin{bmatrix} [cc|c]
			0 & 1 & 0 \\ 0 & 1 & 0
		\end{bmatrix} \xrightarrow{R_2 - R_1} \begin{bmatrix} [cc|c]
			0 & 1 & 0 \\ 0 & 0 & 0
		\end{bmatrix}
	\end{align*} then the solution is
	\begin{align*}
		\textbf{x} = \begin{bmatrix}
			0 \\ 0
		\end{bmatrix} + t\begin{bmatrix}
			1 \\ 0
		\end{bmatrix} (t = 0, 1)
	\end{align*}
	\noindent In $\mathbb{Z}_3$,
	\begin{align*}
		\begin{bmatrix} [cc|c]
		2 & 0 & 1 \\ 1 & 0 & 2
		\end{bmatrix} \xrightarrow{R_2 + R_1} \begin{bmatrix} [cc|c]
		2 & 0 & 1 \\ 0 & 0 & 0
		\end{bmatrix}
	\end{align*} then the solution is
	\begin{align*}
		\textbf{x} = \begin{bmatrix}
			2 \\ 0
		\end{bmatrix} + t\begin{bmatrix}
			0 \\ 1
		\end{bmatrix} (t = 0, 1, 2)
	\end{align*}
	Combining these solutions, we have
	\begin{align*}
		\textbf{x} = \begin{bmatrix}
			2 \\ 0
		\end{bmatrix} + t\begin{bmatrix}
			3 \\ 2
		\end{bmatrix} (t \in \mathbb{Z}_6)
	\end{align*}
	\item \textbf{Exercise 2.3 20}
	\begin{proof}
		\textbf{(a)} Suppose $\textbf{v} \in \textnormal{span}(S)$. Then $\textbf{v}$ can be expressed as linear combination of vectors in $S$, so there exist scalars $c_1, c_2, \cdots, c_k$ such that $\textbf{v} = c_1\textbf{u}_1 + c_2\textbf{u}_2 + \cdots + c_k\textbf{u}_k$. Then $\textbf{v} = c_1\textbf{u}_1 + c_2\textbf{u}_2 + \cdots + c_k\textbf{u}_k + 0\textbf{u}_{k+1} + \cdots + 0\textbf{u}_m$, so $\textbf{v}$ is also a linear combination of vectors in $T$. Therefore, $\textnormal{span}(S) \subset \textnormal{span}(T)$.
		
		\noindent \textbf{(b)} By (a), $\textnormal{span}(S) = \mathbb{R}^n \subset \textnormal{span}(T)$. Also, $\textnormal{span}(T) \subset \mathbb{R}^n$ since span$(T)$ is a set of vectors in \Rn. Therefore, \Rn = span$(T)$.
	\end{proof}
	\item \textbf{Exercise 2.3 21}
	\begin{proof}
		\textbf{(a)} Since each $\textbf{u}_i$ are linear combinations of vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_m$, there exist scalars $c_{i1}, c_{i2}, \cdots, c_{im}$ for each $i$ such that $\textbf{u}_i = c_{i1}\textbf{v}_1 + c_{i2}\textbf{v}_2 + \cdots + c_{im}\textbf{v}_m$. Suppose vector $\textbf{w}$ is in span$(\textbf{u}_1, \textbf{u}_2, \cdots, \textbf{u}_k)$. Since $\textbf{w}$ is a linear combination of $\textbf{u}_1, \textbf{u}_2, \cdots, \textbf{u}_k$, there exist scalars $d_1, d_2, \cdots, d_k$ such that $\textbf{w} = d_1\textbf{u}_1 + d_2\textbf{u}_2 + \cdots + d_k\textbf{u}_k$. Then 
		\begin{align*}
			\textbf{w} &= d_1\textbf{u}_1 + d_2\textbf{u}_2 + \cdots + d_k\textbf{u}_k \\
			&= d_1(c_{11}\textbf{v}_1 + \cdots + c_{1m}\textbf{v}_m) + \cdots + d_k(c_{k1}\textbf{v}_1 + \cdots + c_{km}\textbf{v}_m) \\
			&= (c_{11}d_1 + c_{21}d_2 + \cdots + c_{k1}d_k)\textbf{v}_1 + \cdots + (c_{1m}d_1 + c_{2m}d_2 + \cdots + c_{km}d_k)\textbf{v}_m
		\end{align*} Therefore, $\textbf{w}$ can be expressed as a linear combination of vectors $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_m$, so $\textbf{w} \in$ span($\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_m$).
		
		\noindent \textbf{(b)} The same logic can be applied to prove that span($\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_m$) $\subset$ span($\textbf{u}_1, \textbf{u}_2, \cdots, \textbf{u}_k$). Therefore, span($\textbf{u}_1, \textbf{u}_2, \cdots, \textbf{u}_k$) = span($\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_m$).
		
		\noindent \textbf{(c)} Let $\textbf{v}_1 = \begin{bmatrix}
			1 \\ 0 \\ 0
		\end{bmatrix}, \textbf{v}_2 = \begin{bmatrix}
			1 \\ 1 \\ 0
		\end{bmatrix}, \textbf{v}_3 = \begin{bmatrix}
			1 \\ 1 \\ 1
		\end{bmatrix}$, then $\textbf{v}_1 = \textbf{e}_1$, $\textbf{v}_2 = \textbf{e}_1 + \textbf{e}_2$, and $\textbf{v}_3 = \textbf{e}_1 + \textbf{e}_2 + \textbf{e}_3$. Also, $\textbf{e}_1 = \textbf{v}_1$, $\textbf{e}_2 = \textbf{v}_2 - \textbf{v}_1$, and $\textbf{e}_3 = \textbf{v}_3 - \textbf{v}_2$. Since span($\textbf{e}_1, \textbf{e}_2, \textbf{e}_3$) = $\mathbb{R}^3$, by (b),  span($\textbf{v}_1 , \textbf{v}_2, \textbf{v}_3$) = span($\textbf{e}_1, \textbf{e}_2, \textbf{e}_3$) = $\mathbb{R}^3$.
	\end{proof}
	\item \textbf{Exercise 2.3 44}
	\begin{proof}
		Suppose that vectors $\textbf{v}_1, \textbf{v}_2$ are linearly dependent. Then there exist scalars $c_1, c_2$ such that $c_1\textbf{v}_1 + c_2\textbf{v}_2 = \textbf{0}$.
		
		\noindent (i) $c_1c_2 \neq 0$
		\begin{align*}
			\textbf{v}_1 = -\frac{c_2}{c_1}\textbf{v}_2
		\end{align*}
		\noindent (ii) If not, without loss of generality, $c_1 \neq 0$ and $c_2 = 0$.
		\begin{align*}
			c_1\textbf{v}_1 + c_2\textbf{v}_2 &= \textbf{0} \\
			c_1\textbf{v}_1 &= \textbf{0} \\
			\therefore \textbf{v}_1 &= \textbf{0} = 0\textbf{v}_2
		\end{align*}
	\end{proof}
	\item \textbf{Exercise 2.3 46}
	\begin{proof}
		Given a set of vectors \vn, suppose that vectors $\textbf{v}_{a_1}, \textbf{v}_{a_2}, \cdots, \textbf{v}_{a_m}$ are linearly dependent, where $\{a_1, a_2, \cdots, a_m\} \subset \{1, 2, \cdots n\}$. Then there exists scalars $d_1, d_2, \cdots, d_m$ such that at least one of scalars is nonzero and $d_1\textbf{v}_{a_1} + d_2\textbf{v}_{a_2} + \cdots + d_m\textbf{v}_{a_m} = \textbf{0}$.
		
		\noindent Let 
		\begin{align*}
		c_i = \begin{cases}
			d_j &\mbox{if  } i = a_j \\
			0 &\mbox{if  } i \notin \{a_1, a_2, \cdots, a_m \}
		\end{cases}
		\end{align*} then $c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_n\textbf{v}_n = d_1\textbf{v}_{a_1} + d_2\textbf{v}_{a_2} + \cdots + d_m\textbf{v}_{a_m} = \textbf{0}$. Therefore, the set $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_n$ is linearly dependent. The contrapositive proposition that the subset of linearly independent set of vectors is always linearly independent is also true.
	\end{proof}
	\item \textbf{Exercise 2.3 47}
	\begin{proof}
		We prove this proposition by Exercise 2.3 21.
		
		\noindent (i) For every $\textbf{v}_i \in S'$, $\textbf{v}_i = 0\textbf{v}_1 + \cdots + 1\textbf{v}_i + \cdots + 0\textbf{v}_k + 0\textbf{v}$. Thus, every vectors in $S'$ can be expressed as a linear combination of vectors in $S$.
		
		\noindent (ii) For every $\textbf{v}_i \in S$, $\textbf{v}_i = 0\textbf{v}_1 + \cdots + 1\textbf{v}_i + \cdots + 0\textbf{v}_k$, and $\textbf{v}$ is a linear combination of $\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_k$. Thus, every vectors in $S$ can be expressed as a linear combination of vectors in $S'$.
		
		\noindent By Exercise 2.3 21(b), span($\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_k, \textbf{v}$) = span($\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_k$).
	\end{proof}
	\item \textbf{Exercise 2.3 48}
	\begin{proof}
		Suppose that $\textbf{v} = c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k (c_1 \neq 0)$ and vectors $\textbf{v}, \textbf{v}_2, \cdots, \textbf{v}_k$ are linearly dependent. Then 
	\end{proof}
\end{enumerate}